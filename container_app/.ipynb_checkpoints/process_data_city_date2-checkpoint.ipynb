{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from glob import glob \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_datetime(df, timevar='created_at_h'):\n",
    "    df[timevar] = pd.to_datetime(df[timevar])\n",
    "\n",
    "def fix_token_counter(df):\n",
    "    df.token_counter = df.token_counter.apply(lambda x: Counter(x))  \n",
    "\n",
    "def fix_RT_id(df):\n",
    "    df.RT_id = df.RT_id.astype(str) \n",
    "\n",
    "\n",
    "def convert_floats(df, float_dtype='float32'):\n",
    "    floats = df.select_dtypes(include=['float64']).columns.tolist()\n",
    "    df[floats] = df[floats].astype(float_dtype)\n",
    "    return df\n",
    "\n",
    "def tw_data_files_to_df_csv(files):\n",
    "    '''append and concat data files into a pandas.DataFrame'''\n",
    "    df = []\n",
    "    [ df.append(pd.read_csv(file)) for file in files ]\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def tw_data_files_to_df_csv2(files, frac=0.05, float_dtype=None):\n",
    "    '''append and concat a sample of data into a pandas.DataFrame'''\n",
    "    df = []\n",
    "    [ df.append(pd.read_csv(file, low_memory=True)\n",
    "        .sample(frac=frac, replace=True)) for file in files ]\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    if float_dtype is None: return df\n",
    "    return convert_floats(df, float_dtype)\n",
    "\n",
    "\n",
    "def tw_data_files_to_df_json(files, lines=False):\n",
    "    '''append and concat data files into a pandas.DataFrame'''\n",
    "    df = []\n",
    "    [ df.append(pd.read_json(file, orient='records', lines=lines)) for file in files ]\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def tw_data_files_to_df_json3(files, lines=False, frac=0.05, float_dtype=None, verbose=False):\n",
    "    '''append and concat a sample of data into a pandas.DataFrame'''\n",
    "    df = []\n",
    "    for file in files:\n",
    "        if verbose: print('loading ' + file)\n",
    "        df.append(pd.read_json(file, orient='records', lines=lines)\n",
    "                 .sample(frac=frac, replace=True)) \n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    if float_dtype is None: return df\n",
    "    return convert_floats(df, float_dtype)\n",
    "\n",
    "def keep_recent_files(files, base_timestamp, file_type= '.json', days = 14,\n",
    "                      prefix = 'created_at_'):\n",
    "    timestamps = [pd.Timestamp(file.split(prefix,1)[1]\n",
    "                               .replace(file_type,'').replace('_',' ')) for file in files ]\n",
    "    keep_idx1 = [(base_timestamp - timestamp) <= pd.Timedelta(days, unit='d') for timestamp in timestamps]\n",
    "    return(list(itertools.compress(files,keep_idx1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/kotaminegishi/big_data_training/python/dash_demo1/'\n",
    "data_dest = '/Users/kotaminegishi/big_data_training/python/dash_demo1/'\n",
    "\n",
    "process_datatime = pd.to_datetime(datetime(2020,7,20))\n",
    "process_datatime_d = process_datatime.floor('d')\n",
    "#_dt = latest_datatime.floor('d').to_pydatetime()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_tokens_contain_keyword(df, keyword):\n",
    "    # returns an index indicating whether variable 'tokens' contains keyword\n",
    "    return df.tokens.apply(lambda x: keyword.lower() in x)\n",
    "\n",
    "def mark_tokens_contain_keywords(df, keywords):\n",
    "    idx = [mark_tokens_contain_keyword(df, keyword) for keyword in keywords]\n",
    "    return pd.DataFrame(idx).agg(max).astype(bool)\n",
    "    \n",
    "def mark_tokens_contain_keyword_jointly(df, keywords):\n",
    "    # returns an index indicating whether variable 'tokens' contains keyword\n",
    "    idx = [mark_tokens_contain_keyword(df, keyword) for keyword in keywords]\n",
    "    return pd.DataFrame(idx).agg(min).astype(bool) \n",
    "    \n",
    "\n",
    "def get_columns_json(file):\n",
    "    chunk1 = pd.read_json(file, chunksize=1, orient='records', lines=True)\n",
    "    for d in chunk1:\n",
    "        data1 = d.iloc[0]\n",
    "        break\n",
    "    return list(data1.keys())\n",
    "\n",
    "def get_columns_csv(file):\n",
    "    chunk1 = pd.read_csv(file, chunksize=1)\n",
    "    return list(chunk1.read(1).keys())\n",
    "\n",
    "def df_vars_convert_to_str(df, vars):\n",
    "    for var in vars:\n",
    "        df[var] = df[var].astype(str)\n",
    "        \n",
    "\n",
    "def tw_data_files_to_df_json_filter(files, filter_word, lines=True, float_dtype=None, verbose=False):\n",
    "    '''append and concat filtered data into a pandas.DataFrame'''\n",
    "    if type(filter_word) != list: raise ValueError(\"filter_word must be a list\")\n",
    "\n",
    "    df = []\n",
    "    for file in files:\n",
    "        if verbose: print('loading ' + file)  \n",
    "        if file==files[0]:\n",
    "            columns = get_columns_json(file)\n",
    "            df_null = pd.DataFrame(columns=columns)\n",
    "            \n",
    "        df_file = pd.read_json(file, orient='records', lines=lines)\n",
    "        if (len(filter_word) >1): idx = mark_tokens_contain_keywords(df_file, filter_word)\n",
    "        else: idx = mark_tokens_contain_keyword(df_file, filter_word[0])\n",
    "        df_file_filtered = df_file[idx]\n",
    "        if len(df_file_filtered)>0:\n",
    "            df.append(df_file_filtered)\n",
    "    \n",
    "    if len(df)==0: return df_null\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    if float_dtype is None: return df\n",
    "    return convert_floats(df, float_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_var_in_valuelist(df, var, valuelist):\n",
    "    # returns an index indicating whether variable var is in valuelist\n",
    "    return df[var].apply(lambda x: x in valuelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tw_data_files_to_df_json_match_id(files, varname_id, list_ids,\n",
    "                                      lines=True, float_dtype=None, verbose=False):\n",
    "    '''append and concat filtered data into a pandas.DataFrame'''\n",
    "    if type(list_ids) != list: raise ValueError(\"list_ids must be a list\")\n",
    "\n",
    "    df = []\n",
    "    for file in files:\n",
    "        if verbose: print('loading ' + file)  \n",
    "        if file==files[0]:\n",
    "            columns = get_columns_json(file)\n",
    "            df_null = pd.DataFrame(columns=columns)\n",
    "            \n",
    "        df_file = pd.read_json(file, orient='records', lines=lines)\n",
    "        idx = mark_var_in_valuelist(df_file, varname_id, list_ids)\n",
    "        df_file_filtered = df_file[idx]\n",
    "        if len(df_file_filtered)>0:\n",
    "            df.append(df_file_filtered)\n",
    "    \n",
    "    if len(df)==0: return df_null\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    if float_dtype is None: return df\n",
    "    return convert_floats(df, float_dtype)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Minneapolis','LosAngeles','Denver']\n",
    "city_filterwords = {'Minneapolis': ['Minneapolis', '#Minneapolis','mlps', ['St.', 'Paul']],\n",
    "                    'LosAngeles':['LosAngeles','LA', 'L.A.', '#LA', ['Los', 'Angeles']],\n",
    "                    'Denver': ['Denver', '#Denver']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1    False\n",
       "2     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(city_filterwords['Minneapolis'][0])\n",
    "type(city_filterwords['LosAngeles'][3])\n",
    "idx ={}\n",
    "idx[str('a')] = [True,False,True]\n",
    "idx[str(['a','b'])] = [False,False,True]\n",
    "\n",
    "pd.DataFrame(idx).agg(max, axis=1).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_var_contain_filterwords(df, varname, filterwords):\n",
    "    if type(filterwords) != list: raise ValueError(\"filterwords must be a list\")\n",
    "    idx = {}\n",
    "    for word in filterwords:\n",
    "        if type(word)==str:\n",
    "            idx[str(word)] = df[varname].apply(lambda x: word.lower() in x)\n",
    "        elif type(word)==list:\n",
    "            # assess whether all components of 'word' are jointly present \n",
    "            loc_idx = [df[varname].apply(lambda x: w.lower() in x) for w in word]\n",
    "            idx[str(word)] = pd.DataFrame(loc_idx).agg(min).astype(bool)\n",
    "        else: raise ValueError('each item in filterwords must be str or list')\n",
    "        # assess whether any component of 'filterwords' are present \n",
    "    return pd.DataFrame(idx).agg(max, axis=1).astype(bool)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retweet_files_by_city_json(files, cities, city_filterwords, data_path,\n",
    "                               lines=True, float_dtype='float16', verbose=False):\n",
    "    city_df = {}\n",
    "\n",
    "    for file in files:\n",
    "        if verbose: print('loading ' + file)  \n",
    "        if file==files[0]:\n",
    "            columns = get_columns_json(file)\n",
    "            df_null = pd.DataFrame(columns=columns)\n",
    "            for city in cities:\n",
    "                city_df[city] = []\n",
    "        \n",
    "        df_file = pd.read_json(file, orient='records', lines=lines)\n",
    "        df_vars_convert_to_str(df_file, ['RT_id','user_id','created_at','created_at_h'])\n",
    "        convert_floats(df_file, float_dtype)\n",
    "        \n",
    "        for city in cities:\n",
    "            filter_word = city_filterwords[city]    \n",
    "            idx = mark_var_contain_filterwords(df_file, 'tokens', filter_word)\n",
    "            if sum(idx)>0: city_df[city].append(df_file[idx])\n",
    "    \n",
    "    for city in cities:\n",
    "        if len(city_df[city])==0: city_data = df_null\n",
    "        else: city_data = pd.concat(city_df[city], ignore_index=True)\n",
    "        filename = 'data_cumulative/city_date/' + city + '/retweet/2020_all_retweets' + '.json'\n",
    "        city_data.to_json(data_path + filename, \n",
    "                          orient='records', lines=lines)\n",
    "        print('updated: ', filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#retweet_files_by_city_json(files_retweet, cities, city_filterwords, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_dates(df, varname):\n",
    "    tmp = pd.to_datetime(df[varname]).dt.floor('d')\n",
    "    dates = tmp.unique()\n",
    "    dates_str = [str(date)[:10] for date in dates]\n",
    "    return dates, dates_str\n",
    "\n",
    "def filter_df_by_date(df, varname, date, var_as_string=True):\n",
    "    tmp_df = df\n",
    "    varname_d = varname + '_d'\n",
    "    tmp_df[varname_d] = pd.to_datetime(tmp_df[varname]).dt.floor('d')\n",
    "    filtered_df = tmp_df[tmp_df[varname_d] == pd.to_datetime(date)].drop(columns = [varname_d])\n",
    "    if var_as_string: filtered_df[varname] = filtered_df[varname].astype(str)\n",
    "    return filtered_df\n",
    "\n",
    "def append_to_json(filename, df, lines=True):\n",
    "    df0 = pd.read_json(filename, orient='records', lines=lines)\n",
    "    return df0.append(df)\n",
    "\n",
    "\n",
    "def original_files_by_city_date_json(files, cities, city_filterwords, data_path,\n",
    "                               lines=True, float_dtype='float16', verbose=False,\n",
    "                                    city_type='city', sample_frac = .05):\n",
    "    \n",
    "    if city_type not in ['city','all']: raise ValueError('city_type must be \"city\" or \"all\".')\n",
    "\n",
    "    city_df = {}\n",
    "    city_RT_ids = {}\n",
    "    \n",
    "    \n",
    "    for city in cities:\n",
    "        # retrieve relevant RT_id to match \n",
    "        filename = 'data_cumulative/city_date/' + city + '/retweet/2020_all_retweets' + '.json'\n",
    "        RT_id = pd.read_json(data_path + filename, \n",
    "                             orient='records', lines=True).RT_id.astype(str)\n",
    "        city_RT_ids[city] = list(RT_id)\n",
    "    \n",
    "    for file in files:\n",
    "        if verbose: print('loading ' + file)  \n",
    "        if file==files[0]:\n",
    "            columns = get_columns_json(file)\n",
    "            df_null = pd.DataFrame(columns=columns)\n",
    "            for city in cities:\n",
    "                city_df[city] = []\n",
    "        \n",
    "        df_file = pd.read_json(file, orient='records', lines=lines)\n",
    "        df_vars_convert_to_str(df_file, ['id','RT_id','created_at','created_at_h'])\n",
    "        convert_floats(df_file, float_dtype)\n",
    "        \n",
    "        for city in cities:\n",
    "            if city_type=='city':\n",
    "                if verbose: print('processing data for ' + city)  \n",
    "                filter_word = city_filterwords[city]\n",
    "                # idx1: 'tokens' containing filter_word\n",
    "                idx1 = mark_var_contain_filterwords(df_file, 'tokens', filter_word)\n",
    "                # idx2: relevant retweet's that are matched  \n",
    "                idx2 = mark_var_in_valuelist(df_file, 'RT_id', city_RT_ids[city])\n",
    "                # idx: either idx1 or idx2 being True\n",
    "                idx = pd.DataFrame(data={'idx1':idx1, 'idx2': idx2}).agg(max, axis=1)\n",
    "                print(sum(idx1),sum(idx2), sum(idx))\n",
    "                if sum(idx)>0: city_df[city].append(df_file[idx])\n",
    "            elif city_type=='all':\n",
    "                city_df[city].append(df_file.sample(frac=sample_frac, replace=False))\n",
    "            \n",
    "    for city in cities:\n",
    "        if len(city_df[city])==0: city_data = df_null\n",
    "        else: city_data = pd.concat(city_df[city], ignore_index=True)\n",
    "        dates, dates_str = get_unique_dates(city_data,'created_at_h')\n",
    "        for date in dates_str:\n",
    "            if verbose: print('processing date of ' + date)  \n",
    "            df_date = filter_df_by_date(city_data, 'created_at_h', date)\n",
    "            filename = 'data_cumulative/city_date/' + city + '/original/records_'+ date + '.json'\n",
    "            new_file = glob(data_path + filename)==[]\n",
    "            if new_file:\n",
    "                df_date.to_json(data_path + filename, \n",
    "                              orient='records', lines=lines)\n",
    "                print('created: ', filename)\n",
    "            else:\n",
    "                df_date = append_to_json(data_path + filename, df_date)\n",
    "                df_date.to_json(data_path + filename, \n",
    "                              orient='records', lines=lines)\n",
    "                print('appended: ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/retweet/2020_all_retweets.json\n",
      "/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/retweet/2020_all_retweets.json\n",
      "/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/retweet/2020_all_retweets.json\n",
      "/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/retweet/2020_all_retweets.json\n",
      "/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/retweet/2020_all_retweets.json\n"
     ]
    }
   ],
   "source": [
    "cities_all = ['all_v1','all_v2','all_v3','all_v4','all_v5']\n",
    "\n",
    "files_retweet = glob(data_path + 'data_cumulative/retweet/2020_all_retweets.json')\n",
    "df_retweet = pd.read_json(files_retweet[0], orient='records',lines=True)\n",
    "for c in cities_all:\n",
    "    filename = data_path + 'data_cumulative/city_date/' + c + '/retweet/2020_all_retweets.json'\n",
    "    print(filename)\n",
    "    df_retweet.to_json(filename, orient='records',lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/original/created_at_2020-07-09_11:00:00.json',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/original/created_at_2020-06-30_13:00:00.json',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/original/created_at_2020-07-01_10:00:00.json',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/original/created_at_2020-07-14_16:39:52.json',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/original/created_at_2020-07-15_09:49:43.json',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/original/created_at_2020-07-19_23:42:33.json',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/original/created_at_2020-07-02_15:00:00.json',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/original/created_at_2020-07-02_22:00:00.json',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/original/created_at_2020-06-29_08:00:00.json',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/original/created_at_2020-07-11_09:00:00.json']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_original = glob(data_path + 'data_cumulative/original/*')\n",
    "#original_files_by_city_date_json(files_original[:3], cities, city_filterwords, data_path, verbose=True)\n",
    "files_original[:10]\n",
    "# original_files_by_city_date_json(files_original, cities_all, [], \n",
    "#                                  data_path, verbose=False, \n",
    "#                                  city_type='all', sample_frac=.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def words_files_by_city_date_json(files_words, cities, data_path,\n",
    "                                  process_datetime, process_days = 14,\n",
    "                                  lines=True, verbose=False):\n",
    "    city_df = {}\n",
    "    city_ids = {}\n",
    "    for city in cities:\n",
    "        files_city_original = keep_recent_files(\n",
    "            glob(data_path + \"data_cumulative/city_date/\" + city  + \"/original/*\"),\n",
    "            prefix = 'records_', base_timestamp = process_datatime, days=7)\n",
    "        tmp_ids = []\n",
    "        for file in files_city_original:\n",
    "            # retrieve relevant id to match\n",
    "            if verbose: print('reading ids from ' + file)\n",
    "            ids = pd.read_json(file, orient='records', lines=True).id.astype(str)\n",
    "            tmp_ids.append(ids)\n",
    "        city_ids[city] = list(pd.concat(tmp_ids, ignore_index=True))\n",
    "    \n",
    "    for file in files_words:\n",
    "        if verbose: print('loading ' + file)  \n",
    "        if file==files_words[0]:\n",
    "            columns = get_columns_json(file)\n",
    "            df_null = pd.DataFrame(columns=columns)\n",
    "            for city in cities:\n",
    "                city_df[city] = []\n",
    "        \n",
    "        df_file = pd.read_json(file, orient='records', lines=lines)\n",
    "        df_vars_convert_to_str(df_file, ['id','created_at_h'])\n",
    "        \n",
    "        for city in cities:\n",
    "            if verbose: print('processing data for ' + city)  \n",
    "            # idx: relevant original tweet's that are matched  \n",
    "            idx = mark_var_in_valuelist(df_file, 'id', city_ids[city])\n",
    "            print(sum(idx))\n",
    "            if sum(idx)>0: city_df[city].append(df_file[idx])\n",
    "    \n",
    "    for city in cities:\n",
    "        if len(city_df[city])==0: city_data = df_null\n",
    "        else: city_data = pd.concat(city_df[city], ignore_index=True)\n",
    "        dates, dates_str = get_unique_dates(city_data, 'created_at_h')\n",
    "        for date in dates_str:\n",
    "            if verbose: print('processing date of ' + date)  \n",
    "            df_date = filter_df_by_date(city_data, 'created_at_h', date)\n",
    "            filename = 'data_cumulative/city_date/' + city + '/words/records_'+ date + '.json'\n",
    "            new_file = glob(data_path + filename)==[]\n",
    "            if new_file:\n",
    "                df_date.to_json(data_path + filename, \n",
    "                              orient='records', lines=lines)\n",
    "                print('created: ', filename)\n",
    "            else:\n",
    "                df_date = append_to_json(data_path + filename, df_date)\n",
    "                df_date.to_json(data_path + filename, \n",
    "                              orient='records', lines=lines)\n",
    "                print('appended: ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#words_files_by_city_date_json(files_words[:5], cities, data_path,\n",
    "#                                  process_datetime, process_days = 14,\n",
    "#                                  verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>RT_id</th>\n",
       "      <th>RT_retweet_count</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>text</th>\n",
       "      <th>quoted_text</th>\n",
       "      <th>RT_text</th>\n",
       "      <th>t_co</th>\n",
       "      <th>tags</th>\n",
       "      <th>urls</th>\n",
       "      <th>lang</th>\n",
       "      <th>created_at_h</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1281271899784228864</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>376307465</td>\n",
       "      <td>ClimateCost</td>\n",
       "      <td>1646</td>\n",
       "      <td>1221</td>\n",
       "      <td>Peggy Shepard, @WEACT4EJ1, sheds light on how ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[#blacklivesmatter, #BLM]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>[peggy, shepard,, shed, light, environmental, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1281271900015071232</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1281266982927597568</td>\n",
       "      <td>1</td>\n",
       "      <td>1054013731905196032</td>\n",
       "      <td>Denaldo1mill</td>\n",
       "      <td>8</td>\n",
       "      <td>126</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Black Lives Matter: How can @thecsp members ad...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1281271899985793024</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1281021507083231232</td>\n",
       "      <td>4527</td>\n",
       "      <td>487769182</td>\n",
       "      <td>suzost</td>\n",
       "      <td>22426</td>\n",
       "      <td>20750</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1281271900291776512</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1281271080116563968</td>\n",
       "      <td>7</td>\n",
       "      <td>1264048424501932032</td>\n",
       "      <td>BlackAfroNinjaX</td>\n",
       "      <td>366</td>\n",
       "      <td>372</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Here’s an idea! Arrest the cops who killed #Br...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1281271900593938432</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>294596045</td>\n",
       "      <td>JoseEColon</td>\n",
       "      <td>91</td>\n",
       "      <td>202</td>\n",
       "      <td>@NYCMayor And yet countless of black babies ar...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[#BlackLivesMatter]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>[yet, countless, black, baby, murder, nyc, day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1281281865706545152</td>\n",
       "      <td>2020-07-09 11:39:36</td>\n",
       "      <td>True</td>\n",
       "      <td>1281261692463112193</td>\n",
       "      <td>17</td>\n",
       "      <td>1220730760690372608</td>\n",
       "      <td>Badpanda122</td>\n",
       "      <td>1908</td>\n",
       "      <td>2229</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.\\nThis is convicted #terrorist #SusanRosenbu...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1281281865668796416</td>\n",
       "      <td>2020-07-09 11:39:36</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2999967663</td>\n",
       "      <td>ASRomaEN</td>\n",
       "      <td>575725</td>\n",
       "      <td>15795</td>\n",
       "      <td>#ASRoma is auctioning off a @ChrisSmalling mat...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[https://t.co/NFWT5Vl2cH, https://t.co/LESOsEE...</td>\n",
       "      <td>[#ASRoma, #BlackLivesMatter]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>[#asroma, auction, match-worn, shirt, sleeve, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1281281867090722816</td>\n",
       "      <td>2020-07-09 11:39:36</td>\n",
       "      <td>True</td>\n",
       "      <td>1281278590810808322</td>\n",
       "      <td>424</td>\n",
       "      <td>2981671296</td>\n",
       "      <td>JenDSchneider</td>\n",
       "      <td>520</td>\n",
       "      <td>2785</td>\n",
       "      <td></td>\n",
       "      <td>Our city isn’t just painting the words on Fift...</td>\n",
       "      <td></td>\n",
       "      <td>[https://t.co/VE6MT80qDI]</td>\n",
       "      <td>[#BlackLivesMatter]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>[city, isn’t, paint, word, fifth, avenue., we’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1281281868625821696</td>\n",
       "      <td>2020-07-09 11:39:37</td>\n",
       "      <td>True</td>\n",
       "      <td>1281249485214146562</td>\n",
       "      <td>170</td>\n",
       "      <td>791445473424998400</td>\n",
       "      <td>ga_undrdawg</td>\n",
       "      <td>4823</td>\n",
       "      <td>4558</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1281281870122979328</td>\n",
       "      <td>2020-07-09 11:39:37</td>\n",
       "      <td>True</td>\n",
       "      <td>1281256999259377664</td>\n",
       "      <td>249</td>\n",
       "      <td>1032614128634884096</td>\n",
       "      <td>DeeStengel</td>\n",
       "      <td>372</td>\n",
       "      <td>298</td>\n",
       "      <td></td>\n",
       "      <td>@littledeekay https://t.co/B0hFgi6Ta1</td>\n",
       "      <td></td>\n",
       "      <td>[https://t.co/B0hFgi6Ta1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>2020-07-09 11:00:00</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id          created_at  is_retweet                RT_id  \\\n",
       "0    1281271899784228864 2020-07-09 11:00:00       False                        \n",
       "1    1281271900015071232 2020-07-09 11:00:00        True  1281266982927597568   \n",
       "2    1281271899985793024 2020-07-09 11:00:00        True  1281021507083231232   \n",
       "3    1281271900291776512 2020-07-09 11:00:00        True  1281271080116563968   \n",
       "4    1281271900593938432 2020-07-09 11:00:00       False                        \n",
       "..                   ...                 ...         ...                  ...   \n",
       "995  1281281865706545152 2020-07-09 11:39:36        True  1281261692463112193   \n",
       "996  1281281865668796416 2020-07-09 11:39:36       False                        \n",
       "997  1281281867090722816 2020-07-09 11:39:36        True  1281278590810808322   \n",
       "998  1281281868625821696 2020-07-09 11:39:37        True  1281249485214146562   \n",
       "999  1281281870122979328 2020-07-09 11:39:37        True  1281256999259377664   \n",
       "\n",
       "     RT_retweet_count              user_id        user_name  followers_count  \\\n",
       "0                   0            376307465      ClimateCost             1646   \n",
       "1                   1  1054013731905196032     Denaldo1mill                8   \n",
       "2                4527            487769182           suzost            22426   \n",
       "3                   7  1264048424501932032  BlackAfroNinjaX              366   \n",
       "4                   0            294596045       JoseEColon               91   \n",
       "..                ...                  ...              ...              ...   \n",
       "995                17  1220730760690372608      Badpanda122             1908   \n",
       "996                 0           2999967663         ASRomaEN           575725   \n",
       "997               424           2981671296    JenDSchneider              520   \n",
       "998               170   791445473424998400      ga_undrdawg             4823   \n",
       "999               249  1032614128634884096       DeeStengel              372   \n",
       "\n",
       "     following_count                                               text  \\\n",
       "0               1221  Peggy Shepard, @WEACT4EJ1, sheds light on how ...   \n",
       "1                126                                                      \n",
       "2              20750                                                      \n",
       "3                372                                                      \n",
       "4                202  @NYCMayor And yet countless of black babies ar...   \n",
       "..               ...                                                ...   \n",
       "995             2229                                                      \n",
       "996            15795  #ASRoma is auctioning off a @ChrisSmalling mat...   \n",
       "997             2785                                                      \n",
       "998             4558                                                      \n",
       "999              298                                                      \n",
       "\n",
       "                                           quoted_text  \\\n",
       "0                                                        \n",
       "1                                                        \n",
       "2                                                        \n",
       "3                                                        \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "995                                                      \n",
       "996                                                      \n",
       "997  Our city isn’t just painting the words on Fift...   \n",
       "998                                                      \n",
       "999              @littledeekay https://t.co/B0hFgi6Ta1   \n",
       "\n",
       "                                               RT_text  \\\n",
       "0                                                        \n",
       "1    Black Lives Matter: How can @thecsp members ad...   \n",
       "2                                                        \n",
       "3    Here’s an idea! Arrest the cops who killed #Br...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "995  1.\\nThis is convicted #terrorist #SusanRosenbu...   \n",
       "996                                                      \n",
       "997                                                      \n",
       "998                                                      \n",
       "999                                                      \n",
       "\n",
       "                                                  t_co  \\\n",
       "0                                                   []   \n",
       "1                                                   []   \n",
       "2                                                   []   \n",
       "3                                                   []   \n",
       "4                                                   []   \n",
       "..                                                 ...   \n",
       "995                                                 []   \n",
       "996  [https://t.co/NFWT5Vl2cH, https://t.co/LESOsEE...   \n",
       "997                          [https://t.co/VE6MT80qDI]   \n",
       "998                                                 []   \n",
       "999                          [https://t.co/B0hFgi6Ta1]   \n",
       "\n",
       "                             tags urls lang         created_at_h  \\\n",
       "0       [#blacklivesmatter, #BLM]   []   en  2020-07-09 11:00:00   \n",
       "1                              []   []   en  2020-07-09 11:00:00   \n",
       "2                              []   []   en  2020-07-09 11:00:00   \n",
       "3                              []   []   en  2020-07-09 11:00:00   \n",
       "4             [#BlackLivesMatter]   []   en  2020-07-09 11:00:00   \n",
       "..                            ...  ...  ...                  ...   \n",
       "995                            []   []   en  2020-07-09 11:00:00   \n",
       "996  [#ASRoma, #BlackLivesMatter]   []   en  2020-07-09 11:00:00   \n",
       "997           [#BlackLivesMatter]   []   en  2020-07-09 11:00:00   \n",
       "998                            []   []   en  2020-07-09 11:00:00   \n",
       "999                            []   []   en  2020-07-09 11:00:00   \n",
       "\n",
       "                                                tokens  \n",
       "0    [peggy, shepard,, shed, light, environmental, ...  \n",
       "1                                                   []  \n",
       "2                                                   []  \n",
       "3                                                   []  \n",
       "4    [yet, countless, black, baby, murder, nyc, day...  \n",
       "..                                                 ...  \n",
       "995                                                 []  \n",
       "996  [#asroma, auction, match-worn, shirt, sleeve, ...  \n",
       "997  [city, isn’t, paint, word, fifth, avenue., we’...  \n",
       "998                                                 []  \n",
       "999                                                 []  \n",
       "\n",
       "[1000 rows x 18 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_df = pd.read_json(files_original[0], orient='records',lines=True)\n",
    "tmp_ids = tmp_df.id[:1000] \n",
    "\n",
    "def keep_by_matched_id(df, list_id, varname='id'):\n",
    "    return (df.set_index(varname)\n",
    "            .join(pd.DataFrame(data={varname: list_id}).set_index(varname), how='inner')\n",
    "            .reset_index()\n",
    "            )\n",
    "\n",
    "keep_by_matched_id(tmp_df, list(tmp_ids), varname='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_by_matched_id(df, list_id, varname='id'):\n",
    "    return (df.set_index(varname)\n",
    "            .join(pd.DataFrame(data={varname: list_id}).set_index(varname), how='inner')\n",
    "            .reset_index()\n",
    "            )\n",
    "\n",
    "\n",
    "def files_id_matched_by_city_date_json(\n",
    "    files, cities, data_path, folder, process_datetime, process_days = 14,\n",
    "    file_type ='.json', float_dtype='float16', lines=True, verbose=False):\n",
    "\n",
    "    '''\n",
    "    Looks for recent files in /city_date/[city]/original/*, extract relevant ids,\n",
    "    generate data matched with those ids by city, and create data files  \n",
    "    '''\n",
    "    if file_type not in ['.json', '.csv'] :\n",
    "        raise ValueError('file_type must be either json or csv')\n",
    "            \n",
    "    city_df = {}\n",
    "    city_ids = {}\n",
    "    for city in cities:\n",
    "        files_city_original = keep_recent_files(\n",
    "            glob(data_path + \"data_cumulative/city_date/\" + city  + \"/original/*\"),\n",
    "            prefix = 'records_', file_type= '.json', \n",
    "            base_timestamp = process_datetime, days=process_days)\n",
    "        tmp_ids = []\n",
    "        for file in files_city_original:\n",
    "            # retrieve relevant id to match\n",
    "            if verbose: print('reading ids from ' + file)\n",
    "            ids = pd.read_json(file, orient='records', lines=True).id.astype(str)\n",
    "            tmp_ids.append(ids)\n",
    "        city_ids[city] = list(pd.concat(tmp_ids, ignore_index=True)) if len(tmp_ids)>0 else []\n",
    "    \n",
    "    for file in files:\n",
    "        if verbose: print('loading ' + file)  \n",
    "        if file==files[0]:\n",
    "            columns = get_columns_json(file) if file_type =='.json' else get_columns_csv(file)\n",
    "            df_null = pd.DataFrame(columns=columns)\n",
    "            for city in cities:\n",
    "                city_df[city] = []\n",
    "        \n",
    "        if file_type =='.json': \n",
    "            df_file = pd.read_json(file, orient='records', lines=lines)\n",
    "        elif file_type =='.csv': \n",
    "            df_file = pd.read_csv(file)\n",
    "\n",
    "        df_vars_convert_to_str(df_file, ['id','created_at_h'])\n",
    "        convert_floats(df_file, float_dtype)\n",
    "\n",
    "        for city in cities:\n",
    "            # tmp_df: relevant original tweet's that are matched  \n",
    "            idx = mark_var_in_valuelist(df_file, 'id', city_ids[city])\n",
    "            tmp_df = keep_by_matched_id(df_file, city_ids[city], varname='id')\n",
    "            if verbose: print('matched data for ' + city + ': ' + str(len(tmp_df)) + ' records')  \n",
    "            if len(tmp_df)>0: city_df[city].append(tmp_df)\n",
    "    \n",
    "    for city in cities:\n",
    "        if len(city_df[city])==0: city_data = df_null\n",
    "        else: city_data = pd.concat(city_df[city], ignore_index=True)\n",
    "        dates, dates_str = get_unique_dates(city_data, 'created_at_h')\n",
    "        for date in dates_str:\n",
    "            if verbose: print('processing date of ' + date)  \n",
    "            df_date = filter_df_by_date(city_data, 'created_at_h', date)\n",
    "            filename = 'data_cumulative/city_date/' + city + '/' + folder + '/records_'+ date + file_type\n",
    "            new_file = glob(data_path + filename)==[]\n",
    "            if file_type =='.json': \n",
    "                if not new_file:\n",
    "                    df_date = append_to_json(data_path + filename, df_date)\n",
    "                df_date.to_json(data_path + filename, \n",
    "                              orient='records', lines=lines)\n",
    "            if file_type =='.csv':\n",
    "                mode = 'a' if new_file else 'w'\n",
    "                df_date.to_csv(data_path + filename, index=False, mode=mode)\n",
    "            if new_file: print('created: ', filename)\n",
    "            else: print('appended: ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-17.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-20.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-16.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-13.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-18.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-14.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-17.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-20.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-16.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-13.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-18.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-14.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-17.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-20.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-16.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-13.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-18.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-14.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-17.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-20.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-16.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-13.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-18.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-14.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-17.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-20.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-16.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-13.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-18.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-14.json\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-18_02:26:14.csv\n",
      "matched data for all_v1: 494 records\n",
      "matched data for all_v2: 472 records\n",
      "matched data for all_v3: 469 records\n",
      "matched data for all_v4: 472 records\n",
      "matched data for all_v5: 473 records\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-19_23:42:33.csv\n",
      "matched data for all_v1: 664 records\n",
      "matched data for all_v2: 700 records\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-cfb1aa0dc86a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprocess_datetime\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprocess_days\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     file_type='.csv', verbose=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-1a6cc64f5ffa>\u001b[0m in \u001b[0;36mfiles_id_matched_by_city_date_json\u001b[0;34m(files, cities, data_path, folder, process_datetime, process_days, file_type, float_dtype, lines, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# tmp_df: relevant original tweet's that are matched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmark_var_in_valuelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mtmp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep_by_matched_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matched data for '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' records'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f802c9c6a1ee>\u001b[0m in \u001b[0;36mmark_var_in_valuelist\u001b[0;34m(df, var, valuelist)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmark_var_in_valuelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaluelist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# returns an index indicating whether variable var is in valuelist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvaluelist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f802c9c6a1ee>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmark_var_in_valuelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaluelist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# returns an index indicating whether variable var is in valuelist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvaluelist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "files_sentiments = glob(data_path + 'data_cumulative/sentiments/*')\n",
    "files_sentiments[:5]\n",
    "#files_id_matched_by_city_date_json(\n",
    "#    files_sentiments[:10], cities, data_path, 'sentiments', \n",
    "#    process_datetime, process_days = 14,\n",
    "#    file_type='.csv', verbose=True)\n",
    "\n",
    "\n",
    "files_id_matched_by_city_date_json(\n",
    "    files_sentiments[:5], cities_all, data_path, 'sentiments', \n",
    "    process_datetime= pd.to_datetime(datetime(2020,7,20)), \n",
    "    process_days = 5,\n",
    "    file_type='.csv', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-14.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-14.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-14.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-14.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-14.json\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-18_02:26:14.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-19_23:42:33.csv\n",
      "processing data for all_v1\n",
      "328\n",
      "processing data for all_v2\n",
      "343\n",
      "processing data for all_v3\n",
      "339\n",
      "processing data for all_v4\n",
      "338\n",
      "processing data for all_v5\n",
      "337\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-13_22:55:13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-13_12:38:18.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-19_02:19:37.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-15_22:37:55.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-19_01:17:12.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-14_15:37:12.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-15_13:12:47.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-17_14:31:18.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-17_16:03:16.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-20_05:57:22.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-07_15:17:58.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-18_19:02:57.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-15_15:18:20.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-20_03:52:28.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-19_03:22:02.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-19_15:22:55.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-15_23:40:54.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-12_09:41:01.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-19_09:36:28.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-14_17:42:30.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-14_04:06:04.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-17_10:18:18.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-20_08:02:24.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-16_15:25:25.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-15_18:26:43.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-18_06:35:17.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-19_17:27:35.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-07_15:02:42.csv\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "processing date of 2020-07-19\n",
      "created:  data_cumulative/city_date/all_v1/emotions/records_2020-07-19.csv\n",
      "processing date of 2020-07-19\n",
      "created:  data_cumulative/city_date/all_v2/emotions/records_2020-07-19.csv\n",
      "processing date of 2020-07-19\n",
      "created:  data_cumulative/city_date/all_v3/emotions/records_2020-07-19.csv\n",
      "processing date of 2020-07-19\n",
      "created:  data_cumulative/city_date/all_v4/emotions/records_2020-07-19.csv\n",
      "processing date of 2020-07-19\n",
      "created:  data_cumulative/city_date/all_v5/emotions/records_2020-07-19.csv\n"
     ]
    }
   ],
   "source": [
    "files_emotions = glob(data_path + 'data_cumulative/emotions/*')\n",
    "files_emotions[:5]\n",
    "\n",
    "#files_id_matched_by_city_date_json(\n",
    "#    files_emotions[:10], cities, data_path, 'emotions', \n",
    "#    process_datetime, process_days = 14,\n",
    "#    file_type='.csv', verbose=True)\n",
    "\n",
    "files_id_matched_by_city_date_json(\n",
    "    files_emotions, cities_all, data_path, 'emotions', \n",
    "    process_datatime, process_days = 30,\n",
    "    file_type='.csv', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v1/original/records_2020-07-14.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v2/original/records_2020-07-14.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v3/original/records_2020-07-14.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v4/original/records_2020-07-14.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-15.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-19.json\n",
      "reading ids from /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/all_v5/original/records_2020-07-14.json\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-09_11:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-06-30_13:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-01_10:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-14_16:39:52.json\n",
      "processing data for all_v1\n",
      "586\n",
      "processing data for all_v2\n",
      "598\n",
      "processing data for all_v3\n",
      "595\n",
      "processing data for all_v4\n",
      "590\n",
      "processing data for all_v5\n",
      "582\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-15_09:49:43.json\n",
      "processing data for all_v1\n",
      "807\n",
      "processing data for all_v2\n",
      "809\n",
      "processing data for all_v3\n",
      "804\n",
      "processing data for all_v4\n",
      "813\n",
      "processing data for all_v5\n",
      "809\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-06-26_10:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-19_23:42:33.json\n",
      "processing data for all_v1\n",
      "472\n",
      "processing data for all_v2\n",
      "483\n",
      "processing data for all_v3\n",
      "473\n",
      "processing data for all_v4\n",
      "487\n",
      "processing data for all_v5\n",
      "468\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-02_15:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-02_22:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-06-29_08:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-11_09:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-06_09:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-03_16:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-03_21:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-16_04:55:16.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-15_16:21:07.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-16_23:48:50.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-19_19:32:38.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-16_14:22:26.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-05_10:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-19_20:35:05.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-12_10:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-04_13:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-11_15:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-06-29_23:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-06-29_14:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-11_22:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-07_21:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-02_09:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "loading /Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-07_16:00:00.json\n",
      "processing data for all_v1\n",
      "0\n",
      "processing data for all_v2\n",
      "0\n",
      "processing data for all_v3\n",
      "0\n",
      "processing data for all_v4\n",
      "0\n",
      "processing data for all_v5\n",
      "0\n",
      "processing date of 2020-07-14\n",
      "created:  data_cumulative/city_date/all_v1/words/records_2020-07-14.json\n",
      "processing date of 2020-07-15\n",
      "created:  data_cumulative/city_date/all_v1/words/records_2020-07-15.json\n",
      "processing date of 2020-07-19\n",
      "created:  data_cumulative/city_date/all_v1/words/records_2020-07-19.json\n",
      "processing date of 2020-07-14\n",
      "created:  data_cumulative/city_date/all_v2/words/records_2020-07-14.json\n",
      "processing date of 2020-07-15\n",
      "created:  data_cumulative/city_date/all_v2/words/records_2020-07-15.json\n",
      "processing date of 2020-07-19\n",
      "created:  data_cumulative/city_date/all_v2/words/records_2020-07-19.json\n",
      "processing date of 2020-07-14\n",
      "created:  data_cumulative/city_date/all_v3/words/records_2020-07-14.json\n",
      "processing date of 2020-07-15\n",
      "created:  data_cumulative/city_date/all_v3/words/records_2020-07-15.json\n",
      "processing date of 2020-07-19\n",
      "created:  data_cumulative/city_date/all_v3/words/records_2020-07-19.json\n",
      "processing date of 2020-07-14\n",
      "created:  data_cumulative/city_date/all_v4/words/records_2020-07-14.json\n",
      "processing date of 2020-07-15\n",
      "created:  data_cumulative/city_date/all_v4/words/records_2020-07-15.json\n",
      "processing date of 2020-07-19\n",
      "created:  data_cumulative/city_date/all_v4/words/records_2020-07-19.json\n",
      "processing date of 2020-07-14\n",
      "created:  data_cumulative/city_date/all_v5/words/records_2020-07-14.json\n",
      "processing date of 2020-07-15\n",
      "created:  data_cumulative/city_date/all_v5/words/records_2020-07-15.json\n",
      "processing date of 2020-07-19\n",
      "created:  data_cumulative/city_date/all_v5/words/records_2020-07-19.json\n"
     ]
    }
   ],
   "source": [
    "files_words = glob(data_path + 'data_cumulative/words/*')\n",
    "files_words[:5]\n",
    "\n",
    "#files_id_matched_by_city_date_json(\n",
    "#    files_words[:3], cities, data_path, 'words', \n",
    "#    process_datetime, process_days = 14,\n",
    "#    file_type='.json', verbose=True)\n",
    "\n",
    "files_id_matched_by_city_date_json(\n",
    "    files_words, cities_all, data_path, 'words', \n",
    "    process_datatime, process_days = 30,\n",
    "    file_type='.json', verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_recent_files(files, base_timestamp, file_type= '.json', days = 14, no_newer=False,\n",
    "                      prefix = 'created_at_'):\n",
    "    timestamps = [pd.Timestamp(file.split(prefix,1)[1]\n",
    "                               .replace(file_type,'').replace('_',' ')) for file in files ]\n",
    "    if no_newer: \n",
    "        keep_idx1 = [(base_timestamp - timestamp <= pd.Timedelta(days, unit='d')) & \n",
    "                     (base_timestamp - timestamp >= pd.Timedelta(0, unit='d')) for timestamp in timestamps]\n",
    "    else: \n",
    "        keep_idx1 = [base_timestamp - timestamp <= pd.Timedelta(days, unit='d') for timestamp in timestamps]\n",
    "    return(list(itertools.compress(files,keep_idx1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_all_sentiments2 = keep_recent_files(\n",
    "    glob(data_dest + 'data_cumulative/sentiments/*'), \n",
    "    base_timestamp = pd.to_datetime(datetime(2020, 7, 15, 23, 59)), \n",
    "    file_type= '.csv', days = 3, no_newer=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-13_22:55:13.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-13_12:38:18.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_22:37:55.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_15:37:12.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_13:12:47.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_15:18:20.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_23:40:54.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_17:42:30.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_04:06:04.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_18:26:43.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-13_11:53:58.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-13_10:21:25.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_19:29:29.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_13:29:48.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_17:23:54.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_12:27:04.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_08:16:13.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-13_10:42:27.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_19:47:54.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_06:11:01.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_16:39:52.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_21:35:03.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_12:09:58.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_10:21:35.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_14:32:29.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_16:21:07.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_10:04:41.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_05:08:34.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_00:58:37.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-13_09:30:47.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_08:48:07.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_03:03:38.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_18:45:11.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_09:18:53.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_11:24:20.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_07:13:36.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_09:49:43.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-14_02:01:10.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-13_09:16:23.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_14:15:33.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_11:07:14.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-15_20:32:17.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-13_23:57:44.csv',\n",
       " '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-13_23:45:50.csv']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_all_sentiments2 \n",
    "#pd.to_datetime(datetime(2020, 7, 15, 1)) - pd.to_datetime(datetime(2020, 7, 15, 2)) >= pd.Timedelta(0, unit='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
