{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3client = boto3.client('s3')\n",
    "s3resource = boto3.resource('s3')\n",
    "\n",
    "bucket_name = 'kotasstorage1'\n",
    "bucket = s3resource.Bucket(bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_files = [my_bucket_object.key for my_bucket_object in bucket.objects.all()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9870"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aws_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aws_files[-15:]\n",
    "aws_files[-15]\n",
    "aws_files[-15].find('/data_cumulative/words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools \n",
    "\n",
    "def aws_files_select_type(aws_files, prefix):\n",
    "    idx = [ file.find(prefix) >= 0 for file in aws_files] \n",
    "    return(list(itertools.compress(aws_files, idx)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['app_data/data_cumulative/retweet/2020_all_emotions.csv',\n",
       " 'app_data/data_cumulative/retweet/2020_all_retweets.json',\n",
       " 'app_data/data_cumulative/retweet/2020_all_sentiments.csv',\n",
       " 'app_data/data_cumulative/retweet/2020_all_words.json']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aws_files_app_words = aws_files_select_type(aws_files, \"app_data/data_cumulative/words\")\n",
    "print(len(aws_files_app_words))\n",
    "\n",
    "aws_files_app_original = aws_files_select_type(aws_files, \"app_data/data_cumulative/original\")\n",
    "aws_files_app_original[:2]\n",
    "\n",
    "aws_files_app_sentiments = aws_files_select_type(aws_files, \"app_data/data_cumulative/sentiments\")\n",
    "aws_files_app_sentiments[:2]\n",
    "\n",
    "aws_files_app_retweet = aws_files_select_type(aws_files, \"app_data/data_cumulative/retweet\")\n",
    "aws_files_app_retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['app_data/data_cumulative/words/.DS_Store',\n",
       " 'app_data/data_cumulative/words/created_at_2020-06-26_09:00:00.json',\n",
       " 'app_data/data_cumulative/words/created_at_2020-06-26_12:00:00.json']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aws_files_app_words[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_files_select_dates(aws_files, prefix, time_range_beg, time_range_end):\n",
    "    timestamps = [pd.Timestamp(file.split(prefix)[1][:13]) for file in aws_files]\n",
    "\n",
    "    idx = [(time >= pd.Timestamp(time_range_beg)) & (time < pd.Timestamp(time_range_end)) for time in timestamps]\n",
    "    return(list(itertools.compress(aws_files, idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-06-28 05:00:00')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Timestamp('2020-6-28 05:00:00')\n",
    "pd.Timestamp('2020-6-28 05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231\n",
      "739\n"
     ]
    }
   ],
   "source": [
    "aws_files_original_selected = aws_files_select_dates(aws_files_original, \"BLM_tweet_original_delivery-1-\", \n",
    "                                                     time_range_beg='2020-07-03 14', \n",
    "                                                     time_range_end='2020-07-07 21')\n",
    "print(len(aws_files_original_selected))\n",
    "\n",
    "aws_files_retweet_selected = aws_files_select_dates(aws_files_retweet, \"BLM_tweet_retweet_delivery-1-\", \n",
    "                                                     time_range_beg='2020-07-03 14', \n",
    "                                                     time_range_end='2020-07-07 21')\n",
    "print(len(aws_files_retweet_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/31918960/boto3-to-download-all-files-from-a-s3-bucket\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "def download_dir3(prefix, local, bucket, client, file_type,\n",
    "                  prefix_time, time_range_beg, time_range_end):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    - prefix: pattern to match in s3\n",
    "    - local: local path to folder in which to place files\n",
    "    - bucket: s3 bucket with target contents\n",
    "    - client: initialized s3 client object\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    dirs = []\n",
    "    next_token = ''\n",
    "    base_kwargs = {\n",
    "        'Bucket':bucket,\n",
    "        'Prefix':prefix,\n",
    "    }\n",
    "    while next_token is not None:\n",
    "        kwargs = base_kwargs.copy()\n",
    "        if next_token != '':\n",
    "            kwargs.update({'ContinuationToken': next_token})\n",
    "        results = client.list_objects_v2(**kwargs)\n",
    "        contents = results.get('Contents')\n",
    "        for i in contents:\n",
    "            k = i.get('Key')\n",
    "            \n",
    "            if k.find(prefix_time) >= 0:\n",
    "                timestamp = pd.Timestamp(k.split(prefix_time)[1].split(file_type)[0][:13].replace('_',' '))\n",
    "                if ((timestamp >= pd.Timestamp(time_range_beg)) & (timestamp < pd.Timestamp(time_range_end))): \n",
    "                    print(timestamp)\n",
    "                    if k[-1] != '/':\n",
    "                        keys.append(k)\n",
    "                    else:\n",
    "                        dirs.append(k)\n",
    "        next_token = results.get('NextContinuationToken')\n",
    "    for d in dirs:\n",
    "        dest_pathname = os.path.join(local, d)\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\n",
    "    for k in keys:\n",
    "        dest_pathname = os.path.join(local, k)\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\n",
    "        client.download_file(bucket, k, dest_pathname)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-07-19 18:00:00')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aws_files_app_words[:2]\n",
    "k = 'app_data/data_cumulative/words/created_at_2020-07-19_18:30:08.json'\n",
    "file_type = '.json'\n",
    "\n",
    "pd.Timestamp(\n",
    "    k.split('app_data/data_cumulative/words/created_at_')[1].split(file_type)[0][:13].replace('_',' '))\n",
    "#pd.Timestamp('2020-07-19 18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-13 22:00:00\n",
      "2020-07-13 23:00:00\n",
      "2020-07-13 23:00:00\n",
      "2020-07-14 00:00:00\n",
      "2020-07-14 02:00:00\n",
      "2020-07-14 03:00:00\n",
      "2020-07-14 04:00:00\n",
      "2020-07-14 05:00:00\n",
      "2020-07-14 06:00:00\n",
      "2020-07-14 07:00:00\n",
      "2020-07-14 08:00:00\n",
      "2020-07-14 09:00:00\n",
      "2020-07-14 10:00:00\n",
      "2020-07-14 11:00:00\n",
      "2020-07-14 12:00:00\n",
      "2020-07-14 13:00:00\n",
      "2020-07-14 14:00:00\n",
      "2020-07-14 15:00:00\n",
      "2020-07-14 16:00:00\n",
      "2020-07-14 17:00:00\n",
      "2020-07-14 18:00:00\n",
      "2020-07-14 19:00:00\n",
      "2020-07-15 08:00:00\n",
      "2020-07-15 09:00:00\n",
      "2020-07-15 10:00:00\n",
      "2020-07-15 11:00:00\n",
      "2020-07-15 12:00:00\n",
      "2020-07-15 13:00:00\n",
      "2020-07-15 14:00:00\n",
      "2020-07-15 15:00:00\n",
      "2020-07-15 16:00:00\n",
      "2020-07-15 17:00:00\n",
      "2020-07-15 18:00:00\n",
      "2020-07-15 19:00:00\n",
      "2020-07-15 20:00:00\n",
      "2020-07-15 21:00:00\n",
      "2020-07-15 22:00:00\n",
      "2020-07-15 23:00:00\n",
      "2020-07-16 00:00:00\n",
      "2020-07-16 01:00:00\n",
      "2020-07-16 02:00:00\n",
      "2020-07-16 03:00:00\n",
      "2020-07-16 04:00:00\n",
      "2020-07-16 05:00:00\n",
      "2020-07-16 07:00:00\n",
      "2020-07-16 08:00:00\n",
      "2020-07-16 09:00:00\n",
      "2020-07-16 10:00:00\n",
      "2020-07-16 11:00:00\n",
      "2020-07-16 12:00:00\n",
      "2020-07-16 13:00:00\n",
      "2020-07-16 14:00:00\n",
      "2020-07-16 15:00:00\n",
      "2020-07-16 16:00:00\n",
      "2020-07-16 17:00:00\n",
      "2020-07-16 18:00:00\n",
      "2020-07-16 19:00:00\n",
      "2020-07-16 20:00:00\n",
      "2020-07-16 21:00:00\n",
      "2020-07-16 22:00:00\n",
      "2020-07-16 23:00:00\n",
      "2020-07-17 00:00:00\n",
      "2020-07-17 01:00:00\n",
      "2020-07-17 02:00:00\n",
      "2020-07-17 04:00:00\n",
      "2020-07-17 05:00:00\n",
      "2020-07-17 06:00:00\n",
      "2020-07-17 07:00:00\n",
      "2020-07-17 08:00:00\n",
      "2020-07-17 09:00:00\n",
      "2020-07-17 10:00:00\n",
      "2020-07-17 11:00:00\n",
      "2020-07-17 12:00:00\n",
      "2020-07-17 13:00:00\n",
      "2020-07-17 14:00:00\n",
      "2020-07-17 15:00:00\n",
      "2020-07-17 15:00:00\n",
      "2020-07-17 16:00:00\n",
      "2020-07-17 17:00:00\n",
      "2020-07-17 18:00:00\n",
      "2020-07-17 19:00:00\n",
      "2020-07-17 20:00:00\n",
      "2020-07-17 21:00:00\n",
      "2020-07-17 22:00:00\n",
      "2020-07-17 23:00:00\n",
      "2020-07-18 00:00:00\n",
      "2020-07-18 01:00:00\n",
      "2020-07-18 02:00:00\n",
      "2020-07-18 03:00:00\n",
      "2020-07-18 04:00:00\n",
      "2020-07-18 05:00:00\n",
      "2020-07-18 06:00:00\n",
      "2020-07-18 07:00:00\n",
      "2020-07-18 08:00:00\n",
      "2020-07-18 09:00:00\n",
      "2020-07-18 10:00:00\n",
      "2020-07-18 11:00:00\n",
      "2020-07-18 12:00:00\n",
      "2020-07-18 13:00:00\n",
      "2020-07-18 14:00:00\n",
      "2020-07-18 15:00:00\n",
      "2020-07-18 16:00:00\n",
      "2020-07-18 18:00:00\n",
      "2020-07-18 19:00:00\n",
      "2020-07-18 20:00:00\n",
      "2020-07-18 21:00:00\n",
      "2020-07-18 22:00:00\n",
      "2020-07-18 23:00:00\n",
      "2020-07-19 00:00:00\n",
      "2020-07-19 01:00:00\n",
      "2020-07-19 02:00:00\n",
      "2020-07-19 03:00:00\n",
      "2020-07-19 04:00:00\n",
      "2020-07-19 05:00:00\n",
      "2020-07-19 06:00:00\n",
      "2020-07-19 07:00:00\n",
      "2020-07-19 08:00:00\n",
      "2020-07-19 09:00:00\n",
      "2020-07-19 10:00:00\n",
      "2020-07-19 11:00:00\n",
      "2020-07-19 12:00:00\n",
      "2020-07-19 13:00:00\n",
      "2020-07-19 14:00:00\n",
      "2020-07-19 15:00:00\n",
      "2020-07-19 15:00:00\n",
      "2020-07-19 15:00:00\n",
      "2020-07-19 16:00:00\n",
      "2020-07-19 17:00:00\n",
      "2020-07-19 18:00:00\n",
      "2020-07-19 19:00:00\n",
      "2020-07-19 20:00:00\n",
      "2020-07-19 21:00:00\n",
      "2020-07-19 22:00:00\n",
      "2020-07-19 23:00:00\n",
      "2020-07-20 00:00:00\n",
      "2020-07-20 01:00:00\n",
      "2020-07-20 02:00:00\n",
      "2020-07-20 03:00:00\n",
      "2020-07-20 04:00:00\n",
      "2020-07-20 05:00:00\n",
      "2020-07-20 06:00:00\n",
      "2020-07-20 08:00:00\n",
      "2020-07-20 09:00:00\n",
      "2020-07-13 10:00:00\n",
      "2020-07-13 10:00:00\n",
      "2020-07-13 11:00:00\n",
      "2020-07-13 12:00:00\n",
      "2020-07-13 22:00:00\n",
      "2020-07-13 23:00:00\n",
      "2020-07-13 23:00:00\n",
      "2020-07-14 00:00:00\n",
      "2020-07-14 02:00:00\n",
      "2020-07-14 03:00:00\n",
      "2020-07-14 04:00:00\n",
      "2020-07-14 05:00:00\n",
      "2020-07-14 06:00:00\n",
      "2020-07-14 07:00:00\n",
      "2020-07-14 08:00:00\n",
      "2020-07-14 09:00:00\n",
      "2020-07-14 10:00:00\n",
      "2020-07-14 11:00:00\n",
      "2020-07-14 12:00:00\n",
      "2020-07-14 13:00:00\n",
      "2020-07-14 14:00:00\n",
      "2020-07-14 15:00:00\n",
      "2020-07-14 16:00:00\n",
      "2020-07-14 17:00:00\n",
      "2020-07-14 18:00:00\n",
      "2020-07-14 19:00:00\n",
      "2020-07-15 08:00:00\n",
      "2020-07-15 09:00:00\n",
      "2020-07-15 10:00:00\n",
      "2020-07-15 11:00:00\n",
      "2020-07-15 12:00:00\n",
      "2020-07-15 13:00:00\n",
      "2020-07-15 14:00:00\n",
      "2020-07-15 15:00:00\n",
      "2020-07-15 16:00:00\n",
      "2020-07-15 17:00:00\n",
      "2020-07-15 18:00:00\n",
      "2020-07-15 19:00:00\n",
      "2020-07-15 20:00:00\n",
      "2020-07-15 21:00:00\n",
      "2020-07-15 22:00:00\n",
      "2020-07-15 23:00:00\n",
      "2020-07-16 00:00:00\n",
      "2020-07-16 01:00:00\n",
      "2020-07-16 02:00:00\n",
      "2020-07-16 03:00:00\n",
      "2020-07-16 04:00:00\n",
      "2020-07-16 05:00:00\n",
      "2020-07-16 07:00:00\n",
      "2020-07-16 08:00:00\n",
      "2020-07-16 09:00:00\n",
      "2020-07-16 10:00:00\n",
      "2020-07-16 11:00:00\n",
      "2020-07-16 12:00:00\n",
      "2020-07-16 13:00:00\n",
      "2020-07-16 14:00:00\n",
      "2020-07-16 15:00:00\n",
      "2020-07-16 16:00:00\n",
      "2020-07-16 17:00:00\n",
      "2020-07-16 18:00:00\n",
      "2020-07-16 19:00:00\n",
      "2020-07-16 20:00:00\n",
      "2020-07-16 21:00:00\n",
      "2020-07-16 22:00:00\n",
      "2020-07-16 23:00:00\n",
      "2020-07-17 00:00:00\n",
      "2020-07-17 01:00:00\n",
      "2020-07-17 02:00:00\n",
      "2020-07-17 04:00:00\n",
      "2020-07-17 05:00:00\n",
      "2020-07-17 06:00:00\n",
      "2020-07-17 07:00:00\n",
      "2020-07-17 08:00:00\n",
      "2020-07-17 09:00:00\n",
      "2020-07-17 10:00:00\n",
      "2020-07-17 11:00:00\n",
      "2020-07-17 12:00:00\n",
      "2020-07-17 13:00:00\n",
      "2020-07-17 14:00:00\n",
      "2020-07-17 15:00:00\n",
      "2020-07-17 15:00:00\n",
      "2020-07-17 16:00:00\n",
      "2020-07-17 17:00:00\n",
      "2020-07-17 18:00:00\n",
      "2020-07-17 19:00:00\n",
      "2020-07-17 20:00:00\n",
      "2020-07-17 21:00:00\n",
      "2020-07-17 22:00:00\n",
      "2020-07-17 23:00:00\n",
      "2020-07-18 00:00:00\n",
      "2020-07-18 01:00:00\n",
      "2020-07-18 02:00:00\n",
      "2020-07-18 03:00:00\n",
      "2020-07-18 04:00:00\n",
      "2020-07-18 05:00:00\n",
      "2020-07-18 06:00:00\n",
      "2020-07-18 07:00:00\n",
      "2020-07-18 08:00:00\n",
      "2020-07-18 09:00:00\n",
      "2020-07-18 10:00:00\n",
      "2020-07-18 11:00:00\n",
      "2020-07-18 12:00:00\n",
      "2020-07-18 13:00:00\n",
      "2020-07-18 14:00:00\n",
      "2020-07-18 15:00:00\n",
      "2020-07-18 16:00:00\n",
      "2020-07-18 18:00:00\n",
      "2020-07-18 19:00:00\n",
      "2020-07-18 20:00:00\n",
      "2020-07-18 21:00:00\n",
      "2020-07-18 22:00:00\n",
      "2020-07-18 23:00:00\n",
      "2020-07-19 00:00:00\n",
      "2020-07-19 01:00:00\n",
      "2020-07-19 02:00:00\n",
      "2020-07-19 03:00:00\n",
      "2020-07-19 04:00:00\n",
      "2020-07-19 05:00:00\n",
      "2020-07-19 06:00:00\n",
      "2020-07-19 07:00:00\n",
      "2020-07-19 08:00:00\n",
      "2020-07-19 09:00:00\n",
      "2020-07-19 10:00:00\n",
      "2020-07-19 11:00:00\n",
      "2020-07-19 12:00:00\n",
      "2020-07-19 13:00:00\n",
      "2020-07-19 14:00:00\n",
      "2020-07-19 15:00:00\n",
      "2020-07-19 15:00:00\n",
      "2020-07-19 15:00:00\n",
      "2020-07-19 16:00:00\n",
      "2020-07-19 17:00:00\n",
      "2020-07-19 18:00:00\n",
      "2020-07-19 19:00:00\n",
      "2020-07-19 20:00:00\n",
      "2020-07-19 21:00:00\n",
      "2020-07-19 22:00:00\n",
      "2020-07-19 23:00:00\n",
      "2020-07-20 00:00:00\n",
      "2020-07-20 01:00:00\n",
      "2020-07-20 02:00:00\n",
      "2020-07-20 03:00:00\n",
      "2020-07-20 04:00:00\n",
      "2020-07-20 05:00:00\n",
      "2020-07-20 06:00:00\n",
      "2020-07-20 08:00:00\n",
      "2020-07-20 09:00:00\n",
      "2020-07-13 10:00:00\n",
      "2020-07-13 10:00:00\n",
      "2020-07-13 11:00:00\n",
      "2020-07-13 12:00:00\n",
      "2020-07-13 22:00:00\n",
      "2020-07-13 23:00:00\n",
      "2020-07-13 23:00:00\n",
      "2020-07-14 00:00:00\n",
      "2020-07-14 02:00:00\n",
      "2020-07-14 03:00:00\n",
      "2020-07-14 04:00:00\n",
      "2020-07-14 05:00:00\n",
      "2020-07-14 06:00:00\n",
      "2020-07-14 07:00:00\n",
      "2020-07-14 08:00:00\n",
      "2020-07-14 09:00:00\n",
      "2020-07-14 10:00:00\n",
      "2020-07-14 11:00:00\n",
      "2020-07-14 12:00:00\n",
      "2020-07-14 13:00:00\n",
      "2020-07-14 14:00:00\n",
      "2020-07-14 15:00:00\n",
      "2020-07-14 16:00:00\n",
      "2020-07-14 17:00:00\n",
      "2020-07-14 18:00:00\n",
      "2020-07-14 19:00:00\n",
      "2020-07-15 08:00:00\n",
      "2020-07-15 09:00:00\n",
      "2020-07-15 10:00:00\n",
      "2020-07-15 11:00:00\n",
      "2020-07-15 12:00:00\n",
      "2020-07-15 13:00:00\n",
      "2020-07-15 14:00:00\n",
      "2020-07-15 15:00:00\n",
      "2020-07-15 16:00:00\n",
      "2020-07-15 17:00:00\n",
      "2020-07-15 18:00:00\n",
      "2020-07-15 19:00:00\n",
      "2020-07-15 20:00:00\n",
      "2020-07-15 21:00:00\n",
      "2020-07-15 22:00:00\n",
      "2020-07-15 23:00:00\n",
      "2020-07-16 00:00:00\n",
      "2020-07-16 01:00:00\n",
      "2020-07-16 02:00:00\n",
      "2020-07-16 03:00:00\n",
      "2020-07-16 04:00:00\n",
      "2020-07-16 05:00:00\n",
      "2020-07-16 07:00:00\n",
      "2020-07-16 08:00:00\n",
      "2020-07-16 09:00:00\n",
      "2020-07-16 10:00:00\n",
      "2020-07-16 11:00:00\n",
      "2020-07-16 12:00:00\n",
      "2020-07-16 13:00:00\n",
      "2020-07-16 14:00:00\n",
      "2020-07-16 15:00:00\n",
      "2020-07-16 16:00:00\n",
      "2020-07-16 17:00:00\n",
      "2020-07-16 18:00:00\n",
      "2020-07-16 19:00:00\n",
      "2020-07-16 20:00:00\n",
      "2020-07-16 21:00:00\n",
      "2020-07-16 22:00:00\n",
      "2020-07-16 23:00:00\n",
      "2020-07-17 00:00:00\n",
      "2020-07-17 01:00:00\n",
      "2020-07-17 02:00:00\n",
      "2020-07-17 04:00:00\n",
      "2020-07-17 05:00:00\n",
      "2020-07-17 06:00:00\n",
      "2020-07-17 07:00:00\n",
      "2020-07-17 08:00:00\n",
      "2020-07-17 09:00:00\n",
      "2020-07-17 10:00:00\n",
      "2020-07-17 11:00:00\n",
      "2020-07-17 12:00:00\n",
      "2020-07-17 13:00:00\n",
      "2020-07-17 14:00:00\n",
      "2020-07-17 15:00:00\n",
      "2020-07-17 15:00:00\n",
      "2020-07-17 16:00:00\n",
      "2020-07-17 17:00:00\n",
      "2020-07-17 18:00:00\n",
      "2020-07-17 19:00:00\n",
      "2020-07-17 20:00:00\n",
      "2020-07-17 21:00:00\n",
      "2020-07-17 22:00:00\n",
      "2020-07-17 23:00:00\n",
      "2020-07-18 00:00:00\n",
      "2020-07-18 01:00:00\n",
      "2020-07-18 02:00:00\n",
      "2020-07-18 03:00:00\n",
      "2020-07-18 04:00:00\n",
      "2020-07-18 05:00:00\n",
      "2020-07-18 06:00:00\n",
      "2020-07-18 07:00:00\n",
      "2020-07-18 08:00:00\n",
      "2020-07-18 09:00:00\n",
      "2020-07-18 10:00:00\n",
      "2020-07-18 11:00:00\n",
      "2020-07-18 12:00:00\n",
      "2020-07-18 13:00:00\n",
      "2020-07-18 14:00:00\n",
      "2020-07-18 15:00:00\n",
      "2020-07-18 16:00:00\n",
      "2020-07-18 18:00:00\n",
      "2020-07-18 19:00:00\n",
      "2020-07-18 20:00:00\n",
      "2020-07-18 21:00:00\n",
      "2020-07-18 22:00:00\n",
      "2020-07-18 23:00:00\n",
      "2020-07-19 00:00:00\n",
      "2020-07-19 01:00:00\n",
      "2020-07-19 02:00:00\n",
      "2020-07-19 03:00:00\n",
      "2020-07-19 04:00:00\n",
      "2020-07-19 05:00:00\n",
      "2020-07-19 06:00:00\n",
      "2020-07-19 07:00:00\n",
      "2020-07-19 08:00:00\n",
      "2020-07-19 09:00:00\n",
      "2020-07-19 10:00:00\n",
      "2020-07-19 11:00:00\n",
      "2020-07-19 12:00:00\n",
      "2020-07-19 13:00:00\n",
      "2020-07-19 14:00:00\n",
      "2020-07-19 15:00:00\n",
      "2020-07-19 15:00:00\n",
      "2020-07-19 15:00:00\n",
      "2020-07-19 16:00:00\n",
      "2020-07-19 17:00:00\n",
      "2020-07-19 18:00:00\n",
      "2020-07-19 19:00:00\n",
      "2020-07-19 20:00:00\n",
      "2020-07-19 21:00:00\n",
      "2020-07-19 22:00:00\n",
      "2020-07-19 23:00:00\n",
      "2020-07-20 00:00:00\n",
      "2020-07-20 01:00:00\n",
      "2020-07-20 02:00:00\n",
      "2020-07-20 03:00:00\n",
      "2020-07-20 04:00:00\n",
      "2020-07-20 05:00:00\n",
      "2020-07-20 06:00:00\n",
      "2020-07-20 08:00:00\n",
      "2020-07-20 09:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-13 22:00:00\n",
      "2020-07-13 23:00:00\n",
      "2020-07-13 23:00:00\n",
      "2020-07-14 00:00:00\n",
      "2020-07-14 02:00:00\n",
      "2020-07-14 03:00:00\n",
      "2020-07-14 04:00:00\n",
      "2020-07-14 05:00:00\n",
      "2020-07-14 06:00:00\n",
      "2020-07-14 07:00:00\n",
      "2020-07-14 08:00:00\n",
      "2020-07-14 09:00:00\n",
      "2020-07-14 10:00:00\n",
      "2020-07-14 11:00:00\n",
      "2020-07-14 12:00:00\n",
      "2020-07-14 13:00:00\n",
      "2020-07-14 14:00:00\n",
      "2020-07-14 15:00:00\n",
      "2020-07-14 16:00:00\n",
      "2020-07-14 17:00:00\n",
      "2020-07-14 18:00:00\n",
      "2020-07-14 19:00:00\n",
      "2020-07-15 08:00:00\n",
      "2020-07-15 09:00:00\n",
      "2020-07-15 10:00:00\n",
      "2020-07-15 11:00:00\n",
      "2020-07-15 12:00:00\n",
      "2020-07-15 13:00:00\n",
      "2020-07-15 14:00:00\n",
      "2020-07-15 15:00:00\n",
      "2020-07-15 16:00:00\n",
      "2020-07-15 17:00:00\n",
      "2020-07-15 18:00:00\n",
      "2020-07-15 19:00:00\n",
      "2020-07-15 20:00:00\n",
      "2020-07-15 21:00:00\n",
      "2020-07-15 22:00:00\n",
      "2020-07-15 23:00:00\n",
      "2020-07-16 00:00:00\n",
      "2020-07-16 01:00:00\n",
      "2020-07-16 02:00:00\n",
      "2020-07-16 03:00:00\n",
      "2020-07-16 04:00:00\n",
      "2020-07-16 05:00:00\n",
      "2020-07-16 07:00:00\n",
      "2020-07-16 08:00:00\n",
      "2020-07-16 09:00:00\n",
      "2020-07-16 10:00:00\n",
      "2020-07-16 11:00:00\n",
      "2020-07-16 12:00:00\n",
      "2020-07-16 13:00:00\n",
      "2020-07-16 14:00:00\n",
      "2020-07-16 15:00:00\n",
      "2020-07-16 16:00:00\n",
      "2020-07-16 17:00:00\n",
      "2020-07-16 18:00:00\n",
      "2020-07-16 19:00:00\n",
      "2020-07-16 20:00:00\n",
      "2020-07-16 21:00:00\n",
      "2020-07-16 22:00:00\n",
      "2020-07-16 23:00:00\n",
      "2020-07-17 00:00:00\n",
      "2020-07-17 01:00:00\n",
      "2020-07-17 02:00:00\n",
      "2020-07-17 04:00:00\n",
      "2020-07-17 05:00:00\n",
      "2020-07-17 06:00:00\n",
      "2020-07-17 07:00:00\n",
      "2020-07-17 08:00:00\n",
      "2020-07-17 09:00:00\n",
      "2020-07-17 10:00:00\n",
      "2020-07-17 11:00:00\n",
      "2020-07-17 12:00:00\n",
      "2020-07-17 13:00:00\n",
      "2020-07-17 14:00:00\n",
      "2020-07-17 15:00:00\n",
      "2020-07-17 15:00:00\n",
      "2020-07-17 16:00:00\n",
      "2020-07-17 17:00:00\n",
      "2020-07-17 18:00:00\n",
      "2020-07-17 19:00:00\n",
      "2020-07-17 20:00:00\n",
      "2020-07-17 21:00:00\n",
      "2020-07-17 22:00:00\n",
      "2020-07-17 23:00:00\n",
      "2020-07-18 00:00:00\n",
      "2020-07-18 01:00:00\n",
      "2020-07-18 02:00:00\n",
      "2020-07-18 03:00:00\n",
      "2020-07-18 04:00:00\n",
      "2020-07-18 05:00:00\n",
      "2020-07-18 06:00:00\n",
      "2020-07-18 07:00:00\n",
      "2020-07-18 08:00:00\n",
      "2020-07-18 09:00:00\n",
      "2020-07-18 10:00:00\n",
      "2020-07-18 11:00:00\n",
      "2020-07-18 12:00:00\n",
      "2020-07-18 13:00:00\n",
      "2020-07-18 14:00:00\n",
      "2020-07-18 15:00:00\n",
      "2020-07-18 16:00:00\n",
      "2020-07-18 18:00:00\n",
      "2020-07-18 19:00:00\n",
      "2020-07-18 20:00:00\n",
      "2020-07-18 21:00:00\n",
      "2020-07-18 22:00:00\n",
      "2020-07-18 23:00:00\n",
      "2020-07-19 00:00:00\n",
      "2020-07-19 01:00:00\n",
      "2020-07-19 02:00:00\n",
      "2020-07-19 03:00:00\n",
      "2020-07-19 04:00:00\n",
      "2020-07-19 05:00:00\n",
      "2020-07-19 06:00:00\n",
      "2020-07-19 07:00:00\n",
      "2020-07-19 08:00:00\n",
      "2020-07-19 09:00:00\n",
      "2020-07-19 10:00:00\n",
      "2020-07-19 11:00:00\n",
      "2020-07-19 12:00:00\n",
      "2020-07-19 13:00:00\n",
      "2020-07-19 14:00:00\n",
      "2020-07-19 15:00:00\n",
      "2020-07-19 15:00:00\n",
      "2020-07-19 15:00:00\n",
      "2020-07-19 16:00:00\n",
      "2020-07-19 17:00:00\n",
      "2020-07-19 18:00:00\n",
      "2020-07-19 19:00:00\n",
      "2020-07-19 20:00:00\n",
      "2020-07-19 21:00:00\n",
      "2020-07-19 22:00:00\n",
      "2020-07-19 23:00:00\n",
      "2020-07-20 00:00:00\n",
      "2020-07-20 01:00:00\n",
      "2020-07-20 02:00:00\n",
      "2020-07-20 03:00:00\n",
      "2020-07-20 04:00:00\n",
      "2020-07-20 05:00:00\n",
      "2020-07-20 06:00:00\n",
      "2020-07-20 08:00:00\n",
      "2020-07-20 09:00:00\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'kotasstorage1'\n",
    "dir_prefix_1 = 'app_data' \n",
    "\n",
    "prefix_time_1 = 'app_data/data_cumulative/words/created_at_' \n",
    "prefix_time_2 = 'app_data/data_cumulative/sentiments/created_at_' \n",
    "prefix_time_3 = 'app_data/data_cumulative/emotions/created_at_' \n",
    "prefix_time_4 = 'app_data/data_cumulative/original/created_at_' \n",
    "\n",
    "time_range_beg='2020-07-13 10'\n",
    "time_range_end='2020-07-20 10'\n",
    "\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "download_dir3(dir_prefix_1, \n",
    "              \"/Users/kotaminegishi/big_data_training/python/dash_demo1/new_data\",\n",
    "              bucket_name, s3client,\n",
    "              file_type = '.json',\n",
    "              prefix_time=prefix_time_1, \n",
    "              time_range_beg = time_range_beg,\n",
    "              time_range_end = time_range_end)\n",
    "\n",
    "download_dir3(dir_prefix_1, \n",
    "              \"/Users/kotaminegishi/big_data_training/python/dash_demo1/new_data\",\n",
    "              bucket_name, s3client,\n",
    "              file_type = '.csv',\n",
    "              prefix_time=prefix_time_2, \n",
    "              time_range_beg = time_range_beg,\n",
    "              time_range_end = time_range_end)\n",
    "\n",
    "\n",
    "download_dir3(dir_prefix_1, \n",
    "              \"/Users/kotaminegishi/big_data_training/python/dash_demo1/new_data\",\n",
    "              bucket_name, s3client,\n",
    "              file_type = '.csv',\n",
    "              prefix_time=prefix_time_3, \n",
    "              time_range_beg = time_range_beg,\n",
    "              time_range_end = time_range_end)\n",
    "\n",
    "\n",
    "download_dir3(dir_prefix_1, \n",
    "              \"/Users/kotaminegishi/big_data_training/python/dash_demo1/new_data\",\n",
    "              bucket_name, s3client,\n",
    "              file_type = '.json',\n",
    "              prefix_time=prefix_time_4, \n",
    "              time_range_beg = time_range_beg,\n",
    "              time_range_end = time_range_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Minneapolis','LosAngeles','Denver','Miami','Memphis',\n",
    "          'NewYork','Louisville','Columbus','Atlanta','Washington',\n",
    "          'Chicago','Boston','Oakland','StLouis','Portland',\n",
    "          'Seattle','Houston','SanFrancisco','Philadelphia','Baltimore']\n",
    "\n",
    "    \n",
    "city_filterwords = {\n",
    "    'Minneapolis': ['Minneapolis','mlps', ['St', 'Paul']],\n",
    "    'LosAngeles':['LosAngeles','LA', ['Los', 'Angeles']],\n",
    "    'Denver': ['Denver', 'DEN'],\n",
    "    'Miami': ['Miami'],\n",
    "    'Memphis': ['Memphis'],\n",
    "    'NewYork': ['NewYork',['New','York'], 'NY','NYC','manhattahn'],\n",
    "    'Louisville': ['Louisville'],\n",
    "    'Columbus': ['Columbus'],\n",
    "    'Atlanta': ['Atlanta'],\n",
    "    'Washington': ['Washington','DC','WashingtonDC'],\n",
    "    'Chicago': ['Chicago'],\n",
    "    'Boston': ['Boston'],\n",
    "    'Oakland': ['Oakland'],\n",
    "    'StLouis': ['StLouis',['St','Loius']],\n",
    "    'Portland': ['Portland'],\n",
    "    'Seattle': ['Seattle'],\n",
    "    'Houston': ['Houston'],\n",
    "    'SanFrancisco': ['SanFrancisco','SF',['San','Francisco']],\n",
    "    'Philadelphia': ['Philadelphia'],\n",
    "    'Baltimore': ['Baltimore']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory \n",
    "city = \"Denver\"\n",
    "#city = \"Miami\"\n",
    "\n",
    "# Parent Directory path \n",
    "parent_dir = \"/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/\"\n",
    "  \n",
    "# Path \n",
    "path = os.path.join(parent_dir, city) \n",
    "\n",
    "dir_exist = os.path.isdir(path)\n",
    "\n",
    "\n",
    "if not dir_exist:\n",
    "    # Create the directory \n",
    "    os.mkdir(path) \n",
    "    os.mkdir(path + '/original') \n",
    "    os.mkdir(path + '/retweet') \n",
    "    os.mkdir(path + '/sentiments') \n",
    "    os.mkdir(path + '/emotions')\n",
    "    os.mkdir(path + '/words') \n",
    "\n",
    "    print(\"City_date directory for '%s' created\" %city) \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token_cityname(list_tokens):\n",
    "    tokens = [(token.replace('.','').replace(',','').replace('!','')\n",
    "               .replace('?','').replace('#','')) for token in list_tokens] \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LA', 'Abc']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_token_cityname(['#L.A.','Abc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_name_add_space(name):\n",
    "    caps = [i for i,c in enumerate(name) if c.isupper()]\n",
    "    i = 0\n",
    "    for l in caps[1:]:\n",
    "        name = name[:(l+i)] + ' ' + name[(l+i):]\n",
    "        i = i+1\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Los Angeles Biz District'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_name_add_space('LosAngelesBizDistrict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Minneapolis','LosAngeles','Denver','Miami','Memphis',\n",
    "          'NewYork','Louisville','Columbus','Atlanta','Washington',\n",
    "          'Chicago','Boston','Oakland','StLouis','Portland',\n",
    "          'Seattle','Houston','SanFrancisco','Philadelphia','Baltimore']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cities = []\n",
    "for city in cities:\n",
    "    list_cities.append({'label': city_name_add_space(city), 'value': city})\n",
    "\n",
    "def city_value(city_label_value):\n",
    "    return city_label_value['value']\n",
    "\n",
    "list_cities.sort(key=city_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cities.insert(0,{'label':'--','value':'all'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '--', 'value': 'all'},\n",
       " {'label': '-', 'value': 'all'},\n",
       " {'label': 'Atlanta', 'value': 'Atlanta'},\n",
       " {'label': 'Baltimore', 'value': 'Baltimore'},\n",
       " {'label': 'Boston', 'value': 'Boston'},\n",
       " {'label': 'Chicago', 'value': 'Chicago'},\n",
       " {'label': 'Columbus', 'value': 'Columbus'},\n",
       " {'label': 'Denver', 'value': 'Denver'},\n",
       " {'label': 'Houston', 'value': 'Houston'},\n",
       " {'label': 'Los Angeles', 'value': 'LosAngeles'},\n",
       " {'label': 'Louisville', 'value': 'Louisville'},\n",
       " {'label': 'Memphis', 'value': 'Memphis'},\n",
       " {'label': 'Miami', 'value': 'Miami'},\n",
       " {'label': 'Minneapolis', 'value': 'Minneapolis'},\n",
       " {'label': 'New York', 'value': 'NewYork'},\n",
       " {'label': 'Oakland', 'value': 'Oakland'},\n",
       " {'label': 'Philadelphia', 'value': 'Philadelphia'},\n",
       " {'label': 'Portland', 'value': 'Portland'},\n",
       " {'label': 'San Francisco', 'value': 'SanFrancisco'},\n",
       " {'label': 'Seattle', 'value': 'Seattle'},\n",
       " {'label': 'St Louis', 'value': 'StLouis'},\n",
       " {'label': 'Washington', 'value': 'Washington'}]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '0:00', 'value': 0},\n",
       " {'label': '1:00', 'value': 1},\n",
       " {'label': '2:00', 'value': 2},\n",
       " {'label': '3:00', 'value': 3},\n",
       " {'label': '4:00', 'value': 4},\n",
       " {'label': '5:00', 'value': 5},\n",
       " {'label': '6:00', 'value': 6},\n",
       " {'label': '7:00', 'value': 7},\n",
       " {'label': '8:00', 'value': 8},\n",
       " {'label': '9:00', 'value': 9},\n",
       " {'label': '10:00', 'value': 10},\n",
       " {'label': '11:00', 'value': 11},\n",
       " {'label': '12:00', 'value': 12},\n",
       " {'label': '13:00', 'value': 13},\n",
       " {'label': '14:00', 'value': 14},\n",
       " {'label': '15:00', 'value': 15},\n",
       " {'label': '16:00', 'value': 16},\n",
       " {'label': '17:00', 'value': 17},\n",
       " {'label': '18:00', 'value': 18},\n",
       " {'label': '19:00', 'value': 19},\n",
       " {'label': '20:00', 'value': 20},\n",
       " {'label': '21:00', 'value': 21},\n",
       " {'label': '22:00', 'value': 22},\n",
       " {'label': '23:00', 'value': 23}]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{'label': str(h) + ':00', 'value':h} for h in range(24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-07-13 04:35:00')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "latest_datatime = pd.to_datetime(dt(2020,7,13,4,35))\n",
    "latest_datatime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-07-13 00:00:00'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_datatime = pd.to_datetime(dt(2020,7,13,4,35)).floor('h')\n",
    "latest_datatime_hour = int(str(latest_datatime)[11:13])\n",
    "latest_datatime_d_dt = latest_datatime.floor('d').to_pydatetime()\n",
    "str(latest_datatime_d_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-07-13 01:00:00')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hour = 1\n",
    "hour = '0' + str(hour) if len(str(hour))==1 else str(hour)\n",
    "str(latest_datatime_d_dt)[:10] + ' ' + hour + ':00'\n",
    "tmp = pd.to_datetime(str(latest_datatime_d_dt)[:10] + ' ' + hour)\n",
    "tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-07-13 01:00:00')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(str(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file1 = '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-07_15:02:42.csv'\n",
    "\n",
    "tmp1 = pd.read_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at_h</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1278174883168555008</td>\n",
       "      <td>2020-06-30 21:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1278174884640681984</td>\n",
       "      <td>2020-06-30 21:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1278174884959567872</td>\n",
       "      <td>2020-06-30 21:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1278174886519812096</td>\n",
       "      <td>2020-06-30 21:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1278174887832686592</td>\n",
       "      <td>2020-06-30 21:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id         created_at_h   fear  anger anticip  trust  \\\n",
       "0  1278174883168555008  2020-06-30 21:00:00  False  False   False  False   \n",
       "1  1278174884640681984  2020-06-30 21:00:00  False  False   False  False   \n",
       "2  1278174884959567872  2020-06-30 21:00:00  False  False   False  False   \n",
       "3  1278174886519812096  2020-06-30 21:00:00  False  False   False  False   \n",
       "4  1278174887832686592  2020-06-30 21:00:00  False  False   False  False   \n",
       "\n",
       "  surprise positive negative sadness disgust    joy  \n",
       "0    False     True    False   False   False  False  \n",
       "1    False     True    False   False   False  False  \n",
       "2    False     True    False   False   False  False  \n",
       "3    False     True    False   False   False  False  \n",
       "4    False     True    False   False   False  False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1278174883168555008\n",
       "1         1278174884640681984\n",
       "2         1278174884959567872\n",
       "3         1278174886519812096\n",
       "4         1278174887832686592\n",
       "                 ...         \n",
       "484121    1278332861054177280\n",
       "484122    1278332861787987968\n",
       "484123    1278332861838393344\n",
       "484124    1278332862530551808\n",
       "484125    1278332862710808576\n",
       "Name: id, Length: 484126, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp1.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "file2 = '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-07_15:17:58.csv'\n",
    "tmp2 = pd.read_csv(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at_h</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1278538162915037184</td>\n",
       "      <td>2020-07-01 21:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1278538167474442240</td>\n",
       "      <td>2020-07-01 21:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1278538169319936000</td>\n",
       "      <td>2020-07-01 21:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1278538170456473600</td>\n",
       "      <td>2020-07-01 21:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1278538183693795328</td>\n",
       "      <td>2020-07-01 21:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id         created_at_h   fear  anger anticip  trust  \\\n",
       "0  1278538162915037184  2020-07-01 21:00:00  False   True    True   True   \n",
       "1  1278538167474442240  2020-07-01 21:00:00  False  False   False  False   \n",
       "2  1278538169319936000  2020-07-01 21:00:00  False  False    True  False   \n",
       "3  1278538170456473600  2020-07-01 21:00:00  False  False    True   True   \n",
       "4  1278538183693795328  2020-07-01 21:00:00   True   True    True  False   \n",
       "\n",
       "  surprise positive negative sadness disgust    joy  \n",
       "0     True     True     True   False    True   True  \n",
       "1    False    False     True   False   False  False  \n",
       "2    False     True     True   False   False  False  \n",
       "3     True     True     True   False   False  False  \n",
       "4     True     True    False   False   False   True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         1279249323994292224\n",
       "1         1279249330382094336\n",
       "2         1279249332982755328\n",
       "3         1279249337860620288\n",
       "4         1279249339357986816\n",
       "                 ...         \n",
       "301921    1279551253928960000\n",
       "301922    1279551254277087232\n",
       "301923    1279551256206475264\n",
       "301924    1279551257213050880\n",
       "301925    1279551258714660864\n",
       "Name: id, Length: 301926, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file3 = '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/emotions/created_at_2020-07-07_17:04:05.csv'\n",
    "tmp3 = pd.read_csv(file3)\n",
    "tmp3.id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at_h</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1277436962174099456</td>\n",
       "      <td>2020-06-28 21:00:00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.4199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1277436962970861568</td>\n",
       "      <td>2020-06-28 21:00:00</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.8481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1277436976451530752</td>\n",
       "      <td>2020-06-28 21:00:00</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.5255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1277436976736563200</td>\n",
       "      <td>2020-06-28 21:00:00</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1277436978917670912</td>\n",
       "      <td>2020-06-28 21:00:00</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.1027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id         created_at_h    neg    neu    pos  compound\n",
       "0  1277436962174099456  2020-06-28 21:00:00  0.000  0.896  0.104    0.4199\n",
       "1  1277436962970861568  2020-06-28 21:00:00  0.207  0.326  0.467    0.8481\n",
       "2  1277436976451530752  2020-06-28 21:00:00  0.111  0.710  0.179    0.5255\n",
       "3  1277436976736563200  2020-06-28 21:00:00  0.167  0.833  0.000   -0.2500\n",
       "4  1277436978917670912  2020-06-28 21:00:00  0.079  0.744  0.178    0.1027"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file4 = '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/sentiments/created_at_2020-07-07_11:02:37.csv'\n",
    "tmp4 = pd.read_csv(file4)\n",
    "tmp4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob \n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/\"\n",
    "city = 'Minneapolis'\n",
    "files = glob(data_path + city + '/sentiments/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(files[0].split('records_')[1].split('.csv')[0])  < pd.to_datetime(dt(2020,7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Minneapolis','LosAngeles','Denver','Miami','Memphis',\n",
    "\t          'NewYork','Louisville','Columbus','Atlanta','Washington',\n",
    "\t          'Chicago','Boston','Oakland','StLouis','Portland',\n",
    "\t          'Seattle','Houston','SanFrancisco','Philadelphia','Baltimore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Minneapolis/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Minneapolis/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Minneapolis/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Minneapolis/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Minneapolis/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Minneapolis/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Minneapolis/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/LosAngeles/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/LosAngeles/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/LosAngeles/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/LosAngeles/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/LosAngeles/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/LosAngeles/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/LosAngeles/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Denver/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Denver/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Denver/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Denver/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Denver/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Denver/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Denver/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Miami/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Miami/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Miami/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Miami/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Miami/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Miami/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Miami/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Memphis/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Memphis/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Memphis/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Memphis/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Memphis/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Memphis/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Memphis/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/NewYork/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/NewYork/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/NewYork/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/NewYork/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/NewYork/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/NewYork/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/NewYork/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Louisville/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Louisville/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Louisville/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Louisville/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Louisville/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Louisville/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Louisville/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Columbus/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Columbus/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Columbus/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Columbus/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Columbus/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Columbus/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Columbus/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Atlanta/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Atlanta/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Atlanta/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Atlanta/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Atlanta/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Atlanta/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Atlanta/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Washington/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Washington/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Washington/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Washington/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Washington/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Washington/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Washington/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Chicago/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Chicago/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Chicago/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Chicago/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Chicago/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Chicago/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Chicago/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Boston/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Boston/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Boston/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Boston/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Boston/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Boston/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Boston/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Oakland/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Oakland/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Oakland/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Oakland/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Oakland/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Oakland/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Oakland/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/StLouis/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/StLouis/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/StLouis/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/StLouis/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/StLouis/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/StLouis/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/StLouis/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Portland/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Portland/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Portland/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Portland/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Portland/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Portland/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Portland/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Seattle/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Seattle/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Seattle/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Seattle/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Seattle/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Seattle/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Seattle/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Houston/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Houston/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Houston/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Houston/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Houston/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Houston/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Houston/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/SanFrancisco/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/SanFrancisco/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/SanFrancisco/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/SanFrancisco/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/SanFrancisco/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/SanFrancisco/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/SanFrancisco/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Philadelphia/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Philadelphia/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Philadelphia/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Philadelphia/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Philadelphia/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Philadelphia/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Philadelphia/emotions/records_2020-06-28.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Baltimore/emotions/records_2020-07-03.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Baltimore/emotions/records_2020-07-02.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Baltimore/emotions/records_2020-07-05.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Baltimore/emotions/records_2020-07-04.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Baltimore/emotions/records_2020-07-06.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Baltimore/emotions/records_2020-06-29.csv\n",
      "remove:/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/city_date/Baltimore/emotions/records_2020-06-28.csv\n"
     ]
    }
   ],
   "source": [
    "for city in cities:\n",
    "    files = glob(data_path + city + '/emotions/*')\n",
    "    for file in files:\n",
    "        rm_file = pd.to_datetime(file.split('records_')[1].split('.csv')[0])  < pd.to_datetime(dt(2020,7,7))\n",
    "        if rm_file:\n",
    "            print('remove:' + file)\n",
    "            os.remove(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_columns_json(file):\n",
    "    chunk1 = pd.read_json(file, chunksize=1, orient='records', lines=True)\n",
    "    for d in chunk1:\n",
    "        data1 = d.iloc[0]\n",
    "        break\n",
    "    return list(data1.keys())\n",
    "\n",
    "def get_columns_csv(file):\n",
    "    chunk1 = pd.read_csv(file, chunksize=1)\n",
    "    return list(chunk1.read(1).keys())\n",
    "\n",
    "\n",
    "def load_null_df(data_path):\n",
    "    \n",
    "    null_cum_sentiments = pd.DataFrame(columns = get_columns_csv(\n",
    "                     glob(data_path + 'data_cumulative/sentiments/*')[0]))\n",
    "    \n",
    "    null_cum_emotions = pd.DataFrame(columns = get_columns_csv(\n",
    "                     glob(data_path + 'data_cumulative/emotions/*')[0]))\n",
    "    \n",
    "    null_cum_words = pd.DataFrame(columns = get_columns_json(\n",
    "                     glob(data_path + 'data_cumulative/words/*')[0]))\n",
    "    \n",
    "    null_cum_original = pd.DataFrame(columns = get_columns_json(\n",
    "                     glob(data_path + 'data_cumulative/original/*')[0]))\n",
    "    \n",
    "    null_cum_retweet = pd.DataFrame(columns = get_columns_json(\n",
    "                     glob(data_path + 'data_cumulative/retweet/*')[0]))\n",
    "    \n",
    "    return null_cum_sentiments, null_cum_emotions, null_cum_words, null_cum_original, null_cum_retweet\n",
    "\n",
    "\n",
    "\n",
    "def get_city_data(city, data_path, base_timestamp):\n",
    "\n",
    "        # load recent cumulative data\n",
    "        print('  Loading cumulative data: sentiments and emotions...')\n",
    "        cum_data_path = data_path + \"data_cumulative/city_date/\" + city\n",
    "        curr_data_path = data_path + \"data_current/city/\" + city\n",
    "\n",
    "        files_sentiments = keep_recent_files(glob(cum_data_path + \"/sentiments/*\"),\n",
    "                            base_timestamp=base_timestamp, prefix='records_',\n",
    "                            file_type = '.csv', days=14) \n",
    "        if len(files_sentiments)>0: \n",
    "            cum_sentiments = tw_data_files_to_df_csv(files_sentiments)\n",
    "            cum_sentiments = cum_sentiments.drop_duplicates(subset = 'id')\n",
    "            fix_datetime(cum_sentiments)\n",
    "            stat_sentiments = calc_stat_sentiments(cum_sentiments)\n",
    "        else:\n",
    "            stat_sentiments = null_stat_sentiments\n",
    "            \n",
    "        files_emotions = keep_recent_files(glob(cum_data_path + \"/emotions/*\"),\n",
    "                            base_timestamp=base_timestamp, prefix='records_',\n",
    "                            file_type = '.csv', days=14)\n",
    "        if len(files_emotions)>0:\n",
    "            cum_emotions = tw_data_files_to_df_csv(files_emotions)\n",
    "            cum_emotions = cum_emotions.drop_duplicates(subset = 'id')\n",
    "            fix_datetime(cum_emotions)    \n",
    "            stat_emotions = calc_stat_emotions(cum_emotions)\n",
    "        else:\n",
    "            stat_emotions = null_stat_emotions\n",
    "\n",
    "        print('  Loading cumulative data: words...')\n",
    "        files_words = keep_recent_files(glob(cum_data_path + \"/words/*\"),\n",
    "                                        base_timestamp=base_timestamp, prefix='records_',\n",
    "                                        file_type = '.json', days=7) \n",
    "        \n",
    "        if len(files_words)>0:\n",
    "            cum_words = tw_data_files_to_df_json(files_words, lines=True)\n",
    "            fix_datetime(cum_words)\n",
    "            fix_token_counter(cum_words)\n",
    "        else:\n",
    "            cum_words = null_cum_words\n",
    "\n",
    "        print('  Loading cumulative data: original tweets and retweets...')   \n",
    "        # load recent cumulative data     \n",
    "        files_original = keep_recent_files(glob(cum_data_path + \"/original/*\"),\n",
    "            base_timestamp = base_timestamp, days=7,\n",
    "            prefix='records_', file_type = '.json')\n",
    "        if len(files_original)>0:\n",
    "            cum_original = tw_data_files_to_df_json(files_original, lines=True)\n",
    "            fix_datetime(cum_original)        \n",
    "            fix_RT_id(cum_original)\n",
    "        else:\n",
    "            cum_original = null_cum_original\n",
    "\n",
    "        files_retweet = cum_data_path + \"/retweet/2020_all_retweets.json\"\n",
    "        try: \n",
    "            cum_retweet = pd.read_json(files_retweet, orient='records', lines=True)\n",
    "            fix_datetime(cum_retweet)\n",
    "            fix_RT_id(cum_retweet)\n",
    "        except:\n",
    "            cum_retweet = null_cum_retweet\n",
    "\n",
    "\n",
    "        latest_datatime = cum_original.created_at_h.max()\n",
    "        time_now =  min([latest_datatime, base_timestamp])\n",
    "\n",
    "        cum_data = cumulative_data(cum_ori = cum_original, \n",
    "                                  cum_rt = cum_retweet,\n",
    "                                  cum_words = cum_words,\n",
    "                                  now = time_now\n",
    "                                  )\n",
    "\n",
    "        cum_data.add_words_subsets()\n",
    "        cum_data.add_tweet_subsets()\n",
    "        cum_data.add_user_subsets()\n",
    "\n",
    "        return stat_sentiments, stat_emotions, cum_data.stat_words, cum_data.top_tweets, cum_data.top_users\n",
    "        \n",
    "\n",
    "\n",
    "def update_current_data_city(cities, data_path, base_timestamp):\n",
    "    for city in cities:\n",
    "        print('\\nUpdating current city data files for ' + city)\n",
    "\n",
    "        stat_sentiments, stat_emotions, stat_words, top_tweets, top_users = get_city_data(city, data_path, base_timestamp)\n",
    "        \n",
    "        curr_data_path = data_path + \"data_current/city/\" + city\n",
    "\n",
    "        # update current data: recent cumulative files\n",
    "        stat_sentiments.to_csv(curr_data_path + '/stat_sentiments.csv', index = False)\n",
    "        stat_emotions.to_csv(curr_data_path +  '/stat_emotions.csv', index = False)\n",
    "        print('  Updated current data: stat_sentiments and stat_emotions.')\n",
    "\n",
    "        stat_words.to_json(curr_data_path + '/stat_words.json', orient='records', lines=True)\n",
    "        top_users.to_csv(curr_data_path + '/top_users.csv', index=False)\n",
    "        top_tweets.to_csv(curr_data_path + '/top_tweets.csv', index=False)\n",
    "        print('  Updated current city data: stat_words, top_users, and top_tweets.')\n",
    "\n",
    "        \n",
    "def keep_recent_files(files, base_timestamp, file_type= '.json', days = 14, no_newer=False,\n",
    "                      prefix = 'created_at_'):\n",
    "    timestamps = [pd.Timestamp(file.split(prefix,1)[1]\n",
    "                               .replace(file_type,'').replace('_',' ')) for file in files ]\n",
    "    if no_newer: \n",
    "        keep_idx1 = [(base_timestamp - timestamp <= pd.Timedelta(days, unit='d')) & \n",
    "                     (base_timestamp - timestamp >= pd.Timedelta(0, unit='d')) for timestamp in timestamps]\n",
    "    else: \n",
    "        keep_idx1 = [base_timestamp - timestamp <= pd.Timedelta(days, unit='d') for timestamp in timestamps]\n",
    "    return(list(itertools.compress(files,keep_idx1)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tw_data_files_to_df_csv(files):\n",
    "    '''append and concat data files into a pandas.DataFrame'''\n",
    "    df = []\n",
    "    [ df.append(pd.read_csv(file)) for file in files ]\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def convert_floats(df, float_dtype='float32'):\n",
    "    floats = df.select_dtypes(include=['float64']).columns.tolist()\n",
    "    df[floats] = df[floats].astype(float_dtype)\n",
    "    return df\n",
    "\n",
    "\n",
    "def tw_data_files_to_df_json(files, lines=False):\n",
    "    '''append and concat data files into a pandas.DataFrame'''\n",
    "    df = []\n",
    "    [ df.append(pd.read_json(file, orient='records', lines=lines)) for file in files ]\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/kotaminegishi/big_data_training/python/dash_demo1/'\n",
    "null_cum_sentiments, null_cum_emotions, null_cum_words, null_cum_original, null_cum_retweet = load_null_df(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_time = pd.to_datetime(dt(2020,7,19,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in cities:\n",
    "    # Parent Directory path \n",
    "    parent_dir = data_path + \"data_current/city/\"\n",
    "\n",
    "    # Path \n",
    "    path = os.path.join(parent_dir, city) \n",
    "\n",
    "    dir_exist = os.path.isdir(path)\n",
    "    if not dir_exist:\n",
    "        # Create the directory \n",
    "        os.mkdir(path) \n",
    "\n",
    "def fix_datetime(df, timevar='created_at_h'):\n",
    "    df[timevar] = pd.to_datetime(df[timevar])\n",
    "\n",
    "def fix_token_counter(df):\n",
    "    df.token_counter = df.token_counter.apply(lambda x: Counter(x))  \n",
    "\n",
    "def fix_RT_id(df):\n",
    "    df.RT_id = df.RT_id.astype(str) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizing_helpers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updating current city data files for all_v1\n",
      "  Loading cumulative data: sentiments and emotions...\n",
      "  Loading cumulative data: words...\n",
      "  Loading cumulative data: original tweets and retweets...\n"
     ]
    }
   ],
   "source": [
    "cities_all = ['all_v1', 'all_v2', 'all_v3', 'all_v4', 'all_v5']\n",
    "# update_current_data_city(cities, data_path, base_time)\n",
    "update_current_data_city(cities_all, data_path, base_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2020-07-08 20:00:00'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 =  '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-08_20:00:00.json'\n",
    "tmp1 = pd.read_json(file1, orient='records', lines=True)\n",
    "tmp1.created_at_h.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at_h</th>\n",
       "      <th>token_counter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1281057769123123200</td>\n",
       "      <td>2020-07-08 20:00:00</td>\n",
       "      <td>{'nothing': 1, 'protest': 1, 'around': 1, 'cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1281057774370201600</td>\n",
       "      <td>2020-07-08 20:00:00</td>\n",
       "      <td>{'afternoon,': 1, 'wcu': 1, 'community,': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1281057776106496000</td>\n",
       "      <td>2020-07-08 20:00:00</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1281057777167843328</td>\n",
       "      <td>2020-07-08 20:00:00</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1281057777817804800</td>\n",
       "      <td>2020-07-08 20:00:00</td>\n",
       "      <td>{'report': 1, 'get': 2, 'people': 2, 'like': 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12757</th>\n",
       "      <td>1281046363434889216</td>\n",
       "      <td>2020-07-08 20:00:00</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12758</th>\n",
       "      <td>1281046366249086976</td>\n",
       "      <td>2020-07-08 20:00:00</td>\n",
       "      <td>{'\"until': 1, 'educate': 1, 'entire': 1, 'huma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12759</th>\n",
       "      <td>1281046367247441920</td>\n",
       "      <td>2020-07-08 20:00:00</td>\n",
       "      <td>{'spirit!': 1, 'hell-bent': 1, 'revive': 1, 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12760</th>\n",
       "      <td>1281046368409141248</td>\n",
       "      <td>2020-07-08 20:00:00</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12761</th>\n",
       "      <td>1281046369092792320</td>\n",
       "      <td>2020-07-08 20:00:00</td>\n",
       "      <td>{'next': 1, 'leader': 1, 'gop': 2, 'trump': 1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12762 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id         created_at_h  \\\n",
       "0      1281057769123123200  2020-07-08 20:00:00   \n",
       "1      1281057774370201600  2020-07-08 20:00:00   \n",
       "2      1281057776106496000  2020-07-08 20:00:00   \n",
       "3      1281057777167843328  2020-07-08 20:00:00   \n",
       "4      1281057777817804800  2020-07-08 20:00:00   \n",
       "...                    ...                  ...   \n",
       "12757  1281046363434889216  2020-07-08 20:00:00   \n",
       "12758  1281046366249086976  2020-07-08 20:00:00   \n",
       "12759  1281046367247441920  2020-07-08 20:00:00   \n",
       "12760  1281046368409141248  2020-07-08 20:00:00   \n",
       "12761  1281046369092792320  2020-07-08 20:00:00   \n",
       "\n",
       "                                           token_counter  \n",
       "0      {'nothing': 1, 'protest': 1, 'around': 1, 'cou...  \n",
       "1      {'afternoon,': 1, 'wcu': 1, 'community,': 1, '...  \n",
       "2                                                     {}  \n",
       "3                                                     {}  \n",
       "4      {'report': 1, 'get': 2, 'people': 2, 'like': 3...  \n",
       "...                                                  ...  \n",
       "12757                                               None  \n",
       "12758  {'\"until': 1, 'educate': 1, 'entire': 1, 'huma...  \n",
       "12759  {'spirit!': 1, 'hell-bent': 1, 'revive': 1, 'r...  \n",
       "12760                                               None  \n",
       "12761  {'next': 1, 'leader': 1, 'gop': 2, 'trump': 1,...  \n",
       "\n",
       "[12762 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1280834012840374272\n",
       "1        1280834014551687168\n",
       "2        1280834015168061440\n",
       "3        1280834016891944960\n",
       "4        1280834018011906048\n",
       "                ...         \n",
       "19975    1280838385888870400\n",
       "19976    1280838386841014272\n",
       "19977    1280838387587592192\n",
       "19978    1280838396080820224\n",
       "19979    1280838397796499456\n",
       "Name: id, Length: 19980, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file2 =  '/Users/kotaminegishi/big_data_training/python/dash_demo1/data_cumulative/words/created_at_2020-07-08_06:00:00.json'\n",
    "tmp2 = pd.read_json(file2, orient='records', lines=True)\n",
    "\n",
    "tmp2.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2020-07-08 06:00:00'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2.created_at_h.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
